{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zongheng/anaconda/envs/ray-0321/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "# TODO(zongheng): read from argparse.\n",
    "# TODO(zongheng): visualization via tensorboard.\n",
    "# TODO(zongheng): print some prediction from test set\n",
    "\n",
    "# Toy dataset.\n",
    "TRAIN_PATH = \"data/train-5rel-10numAttrs-26totalAttrs-100maxTblSize-100buckets-30.csv\"\n",
    "TEST_PATH = \"data/test-5rel-10numAttrs-26totalAttrs-100maxTblSize-100buckets-30.csv\"\n",
    "NUM_FIELDS = 38\n",
    "net = [64]\n",
    "lr = 5e-4\n",
    "batch_size = 1\n",
    "num_steps = 7000\n",
    "\n",
    "# 1000 examples in training.\n",
    "# Min train loss < 0.6\n",
    "TRAIN_PATH = \"data/train-5rel-10numAttrs-17totalAttrs-100maxTblSize-100buckets-3000.csv\"\n",
    "TEST_PATH = \"data/test-5rel-10numAttrs-17totalAttrs-100maxTblSize-100buckets-3000.csv\"\n",
    "NUM_FIELDS = 29\n",
    "net = [64]\n",
    "lr = 3e-3\n",
    "batch_size = 128\n",
    "num_steps = 5500\n",
    "\n",
    "\n",
    "# With gaussian weights.\n",
    "PATH = \"5rel-10numAttrs-24totalAttrs-100maxTblSize-10buckets-300.csv\"\n",
    "NUM_FIELDS = 36\n",
    "TRAIN_PATH = \"data/train-{}\".format(PATH)\n",
    "TEST_PATH = \"data/test-{}\".format(PATH)\n",
    "net = [128, 32, 64]\n",
    "lr = 3e-3\n",
    "lr = 1e-2\n",
    "batch_size = 50\n",
    "num_steps = 5500\n",
    "\n",
    "# With gaussian weights.\\\n",
    "PATH = \"5rel-10numAttrs-22totalAttrs-100maxTblSize-10buckets-3000.csv\"\n",
    "NUM_FIELDS = 34\n",
    "TRAIN_PATH = \"data/train-{}\".format(PATH)\n",
    "TEST_PATH = \"data/test-{}\".format(PATH)\n",
    "net = [64,32]; lr = 1e-2\n",
    "net = [64, 16, 64]; lr = 5e-3\n",
    "net = [64]; lr =  5e-4\n",
    "batch_size = 500\n",
    "num_steps = 5500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.float32)\n",
      "(TensorShape([Dimension(None), Dimension(33)]), TensorShape([Dimension(None)]))\n",
      "(tf.float32, tf.float32)\n",
      "(TensorShape([Dimension(None), Dimension(33)]), TensorShape([Dimension(None)]))\n"
     ]
    }
   ],
   "source": [
    "def DatasetFromPath(pathname):\n",
    "    dataset = tf.data.TextLineDataset(pathname)\n",
    "\n",
    "    def ParseLine(line):\n",
    "        fields = tf.decode_csv(line, [[0.0]] * NUM_FIELDS)\n",
    "        # Last number is label.\n",
    "        return tf.reshape(tf.stack(fields[:-1]), [-1]), fields[-1]\n",
    "\n",
    "    def MeanCenterBatch(feat, lab):\n",
    "        feat_mean, feat_var = tf.nn.moments(feat, axes=[0])\n",
    "        lab_mean, lab_var = tf.nn.moments(lab, axes=[0])\n",
    "#         feat = (feat - feat_mean) / (tf.sqrt(feat_var) + 1e-6)\n",
    "#         lab = (lab - lab_mean) / (tf.sqrt(lab_var) + 1e-6)\n",
    "        \n",
    "        feat = (feat - feat_mean) / (feat_var + 1e-3)\n",
    "        lab = (lab - lab_mean) / (lab_var + 1e-3)\n",
    "        feat = tf.Print(feat, [feat_mean, feat_var], message='feat mean/var: ')\n",
    "        lab = tf.Print(lab, [lab_mean, lab_var], message='lab mean/var: ')\n",
    "        return feat, lab\n",
    "    \n",
    "    # K-point dataset, for sanity checking.\n",
    "    K = 2\n",
    "    dataset = dataset.take(K).map(ParseLine)\n",
    "    dataset = dataset.shuffle(20000, reshuffle_each_iteration=False) \\\n",
    "        .batch(K) \\\n",
    "        .repeat()\n",
    "\n",
    "#     dataset = dataset.cache().map(ParseLine)\n",
    "#     dataset = dataset.shuffle(20000, reshuffle_each_iteration=True) \\\n",
    "#         .repeat() \\\n",
    "#         .batch(batch_size)\n",
    "# #         .map(MeanCenterBatch) \n",
    "\n",
    "    print(dataset.output_types)\n",
    "    print(dataset.output_shapes)\n",
    "    return dataset\n",
    "\n",
    "# Train.\n",
    "dataset = DatasetFromPath(TRAIN_PATH)\n",
    "iterator = tf.data.Iterator.from_structure(\n",
    "    dataset.output_types, dataset.output_shapes)\n",
    "iterator_init = iterator.make_initializer(dataset)\n",
    "# Test.\n",
    "test_dataset = DatasetFromPath(TEST_PATH)\n",
    "test_iterator_init = iterator.make_initializer(test_dataset)\n",
    "# Shared.\n",
    "feature_batch, label_batch = iterator.get_next()\n",
    "\n",
    "    \n",
    "\n",
    "# mean, var = tf.nn.moments(feature_batch, axes=[0])\n",
    "# feature_batch = (feature_batch - mean) / tf.sqrt(var)\n",
    "# mean, var = tf.nn.moments(label_batch, axes=[0])\n",
    "# label_batch = (label_batch - mean) / var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zongheng/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "INFO:tensorflow:Summary name Train Loss is illegal; using Train_Loss instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0 is illegal; using dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0 is illegal; using dense/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0 is illegal; using dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0 is illegal; using dense_1/bias_0 instead.\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Graph definition.\n",
    "for i in range(len(net)):\n",
    "    dense = tf.layers.dense(\n",
    "        dense if i > 0 else feature_batch,\n",
    "        net[i],\n",
    "#         kernel_initializer=tf.truncated_normal_initializer(stddev=20.0),\n",
    "#         kernel_initializer=tf.initializers.variance_scaling(scale=10.0),\n",
    "#         kernel_initializer=tf.initializers.orthogonal(),\n",
    "#         kernel_initializer=tf.initializers.uniform_unit_scaling(),\n",
    "#         kernel_initializer=tf.initializers.random_normal(stddev=10),\n",
    "        kernel_initializer=tf.initializers.random_uniform(-100, 100),\n",
    "#         kernel_initializer=tf.initializers.zeros(),\n",
    "#         bias_initializer=tf.initializers.zeros(),\n",
    "        activation=tf.nn.sigmoid,\n",
    "#         activation=tf.nn.relu,\n",
    "#         activation=tf.nn.tanh,\n",
    "    )\n",
    "out = tf.layers.dense(\n",
    "    dense,\n",
    "    1,\n",
    "#         kernel_initializer=tf.truncated_normal_initializer(stddev=20.0),\n",
    "#         kernel_initializer=tf.initializers.variance_scaling(scale=10.0),\n",
    "#         kernel_initializer=tf.initializers.orthogonal(),\n",
    "#         kernel_initializer=tf.initializers.uniform_unit_scaling(),\n",
    "#         kernel_initializer=tf.initializers.random_normal(stddev=10),\n",
    "        kernel_initializer=tf.initializers.random_uniform(-100, 100),\n",
    "#         kernel_initializer=tf.initializers.zeros(),\n",
    "#         bias_initializer=tf.initializers.zeros(),\n",
    ")\n",
    "\n",
    "\n",
    "all_vars = tf.trainable_variables()\n",
    "l1_regularizer = tf.contrib.layers.l1_regularizer(scale=0.05)\n",
    "regularization_penalty = tf.contrib.layers.apply_regularization(l1_regularizer, all_vars)\n",
    "\n",
    "# loss = tf.reduce_mean(tf.square(out - label_batch))\n",
    "loss = tf.reduce_mean(tf.square(out - label_batch)) #+ regularization_penalty\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "train_step = optimizer.minimize(\n",
    "    loss, global_step=global_step)\n",
    "\n",
    "\n",
    "# For visualization.\n",
    "tf.summary.scalar('Train Loss', loss)\n",
    "for v in all_vars:\n",
    "    tf.summary.histogram(v.name, v)\n",
    "all_vars_norms = [tf.norm(v) for v in all_vars]\n",
    "print(len(all_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "global_step 1 , loss 93054.78\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.  94.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   1.\n",
      "    0.   0.   0.   0.  53.]]\n",
      " true labels train [1. 7.]\n",
      " pred labels train [-306.89664 -295.05658]\n",
      " all vars norms [2622.2766, 0.0, 470.3299, 0.0005]\n",
      "global_step 1001 , loss 84364.09\n",
      "global_step 2001 , loss 76237.414\n",
      "global_step 3001 , loss 68611.81\n",
      "global_step 4001 , loss 61450.703\n",
      "global_step 5001 , loss 54725.195\n",
      "global_step 6001 , loss 48404.83\n",
      "global_step 7001 , loss 42507.293\n",
      "global_step 8001 , loss 37024.008\n",
      "global_step 9001 , loss 31930.72\n",
      "global_step 10001 , loss 27242.3\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.  94.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   1.\n",
      "    0.   0.   0.   0.  53.]]\n",
      " true labels train [1. 7.]\n",
      " pred labels train [-164.53278 -157.44133]\n",
      " all vars norms [2622.2766, 0.0, 468.0008, 4.7456784]\n",
      "global_step 11001 , loss 22959.375\n",
      "global_step 12001 , loss 19058.523\n",
      "global_step 13001 , loss 15559.912\n",
      "global_step 14001 , loss 12439.93\n",
      "global_step 15001 , loss 9701.745\n",
      "global_step 16001 , loss 7339.1733\n",
      "global_step 17001 , loss 5342.404\n",
      "global_step 18001 , loss 3700.0605\n",
      "global_step 19001 , loss 2398.6648\n",
      "global_step 20001 , loss 1419.9424\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.  94.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   1.\n",
      "    0.   0.   0.   0.  53.]]\n",
      " true labels train [1. 7.]\n",
      " pred labels train [-34.914867 -32.15962 ]\n",
      " all vars norms [2622.2766, 0.0, 467.1296, 9.066278]\n",
      "global_step 21001 , loss 737.766\n",
      "global_step 22001 , loss 315.6746\n",
      "global_step 23001 , loss 101.40944\n",
      "global_step 24001 , loss 24.835447\n",
      "global_step 25001 , loss 10.329212\n",
      "global_step 26001 , loss 9.357249\n",
      "global_step 27001 , loss 9.240736\n",
      "global_step 28001 , loss 9.131292\n",
      "global_step 29001 , loss 9.048715\n",
      "global_step 30001 , loss 9.009831\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.  94.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   1.\n",
      "    0.   0.   0.   0.  53.]]\n",
      " true labels train [1. 7.]\n",
      " pred labels train [3.9034338 4.101676 ]\n",
      " all vars norms [2622.2766, 0.0, 467.09885, 10.331014]\n",
      "global_step 31001 , loss 9.000714\n",
      "global_step 32001 , loss 9.00001\n",
      "global_step 33001 , loss 9.0\n",
      "global_step 34001 , loss 9.0\n",
      "global_step 35001 , loss 9.0\n",
      "global_step 36001 , loss 9.0\n",
      "global_step 37001 , loss 9.0\n",
      "global_step 38001 , loss 9.0\n",
      "global_step 39001 , loss 9.0\n",
      "global_step 40001 , loss 9.0\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.  94.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   1.\n",
      "    0.   0.   0.   0.  53.]]\n",
      " true labels train [1. 7.]\n",
      " pred labels train [4.0000277 3.9999971]\n",
      " all vars norms [2622.2766, 0.0, 467.09863, 10.328275]\n",
      "global_step 41001 , loss 9.0\n",
      "global_step 42001 , loss 9.0\n",
      "global_step 43001 , loss 9.0\n",
      "global_step 44001 , loss 9.0\n",
      "global_step 45001 , loss 9.0\n",
      "global_step 46001 , loss 9.0\n",
      "global_step 47001 , loss 9.0\n",
      "global_step 48001 , loss 9.0\n",
      "global_step 49001 , loss 9.0\n",
      "global_step 50001 , loss 9.0\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.  94.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   1.\n",
      "    0.   0.   0.   0.  53.]]\n",
      " true labels train [1. 7.]\n",
      " pred labels train [4. 4.]\n",
      " all vars norms [2622.2766, 0.0, 467.09863, 10.328308]\n",
      "global_step 51001 , loss 9.0\n",
      "global_step 52001 , loss 9.0\n",
      "global_step 53001 , loss 9.0\n",
      "global_step 54001 , loss 9.0\n",
      "global_step 55001 , loss 9.0\n",
      "global_step 56001 , loss 9.0\n",
      "global_step 57001 , loss 9.0\n",
      "global_step 58001 , loss 9.0\n",
      "global_step 59001 , loss 9.0\n",
      "global_step 60001 , loss 9.0\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.  94.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   1.\n",
      "    0.   0.   0.   0.  53.]]\n",
      " true labels train [1. 7.]\n",
      " pred labels train [4. 4.]\n",
      " all vars norms [2622.2766, 0.0, 467.09863, 10.328308]\n",
      "global_step 61001 , loss 9.0\n",
      "global_step 62001 , loss 9.0\n",
      "global_step 63001 , loss 9.0\n",
      "global_step 64001 , loss 9.0\n",
      "global_step 65001 , loss 9.0\n"
     ]
    }
   ],
   "source": [
    "# Train.\n",
    "sess_args = {\n",
    "#              'checkpoint_dir': os.getcwd(),\n",
    "#              'save_checkpoint_secs': 60, \n",
    "             'save_summaries_secs': 1,\n",
    "             'log_step_count_steps': 5000,\n",
    "             'hooks': [tf.train.NanTensorHook(loss),\n",
    "                       tf.train.StopAtStepHook(last_step=2000000),\n",
    "#                       tf.train.LoggingTensorHook([global_step, loss], every_n_iter=500)\n",
    "                      ],\n",
    "            }\n",
    "\n",
    "loss_vals = collections.deque(maxlen=40)\n",
    "i = 0\n",
    "with tf.train.MonitoredTrainingSession(**sess_args) as sess:\n",
    "    sess.run(iterator_init)\n",
    "    while not sess.should_stop():\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     running_test_loss = 0.0\n",
    "#     while True:\n",
    "        _, loss_val, global_step_val, true_feat_train, true_labels_train, pred_labels_train = sess.run(\n",
    "            [train_step, loss, global_step,  feature_batch, label_batch, out])\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "#             sess.run(test_iterator_init)\n",
    "#             test_loss_val, actual_labels, predicted_labels = sess.run([loss, label_batch, out])\n",
    "\n",
    "#             loss_vals.append(test_loss_val)\n",
    "#             avg_test_loss = np.mean(loss_vals)\n",
    "#             print('global_step', global_step_val, ', loss', loss_val, ', avg test loss', avg_test_loss)\n",
    "            print('global_step', global_step_val, ', loss', loss_val)\n",
    "            if i % 10000 == 0:\n",
    "#                 print(' true labels', actual_labels[:20].reshape(-1,))\n",
    "#                 print(' pred labels', predicted_labels[:20].reshape(-1,))\n",
    "                print(' true feats train', true_feat_train[:2])\n",
    "                print(' true labels train', true_labels_train[:10].reshape(-1,))\n",
    "                print(' pred labels train', pred_labels_train[:10].reshape(-1,))\n",
    "                print(' all vars norms', sess.run(all_vars_norms))\n",
    "        i += 1\n",
    "#         if i >= 10000:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
