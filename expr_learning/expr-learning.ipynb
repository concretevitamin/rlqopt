{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zongheng/anaconda/envs/ray-0321/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "# TODO(zongheng): read from argparse.\n",
    "# TODO(zongheng): visualization via tensorboard.\n",
    "# TODO(zongheng): print some prediction from test set\n",
    "\n",
    "# Toy dataset.\n",
    "TRAIN_PATH = \"data/train-5rel-10numAttrs-26totalAttrs-100maxTblSize-100buckets-30.csv\"\n",
    "TEST_PATH = \"data/test-5rel-10numAttrs-26totalAttrs-100maxTblSize-100buckets-30.csv\"\n",
    "NUM_FIELDS = 38\n",
    "net = [64]\n",
    "lr = 5e-4\n",
    "batch_size = 1\n",
    "num_steps = 7000\n",
    "\n",
    "# 1000 examples in training.\n",
    "# Min train loss < 0.6\n",
    "TRAIN_PATH = \"data/train-5rel-10numAttrs-17totalAttrs-100maxTblSize-100buckets-3000.csv\"\n",
    "TEST_PATH = \"data/test-5rel-10numAttrs-17totalAttrs-100maxTblSize-100buckets-3000.csv\"\n",
    "NUM_FIELDS = 29\n",
    "net = [64]\n",
    "lr = 3e-3\n",
    "batch_size = 128\n",
    "num_steps = 5500\n",
    "\n",
    "\n",
    "# With gaussian weights.\n",
    "PATH = \"5rel-10numAttrs-24totalAttrs-100maxTblSize-10buckets-300.csv\"\n",
    "NUM_FIELDS = 36\n",
    "TRAIN_PATH = \"data/train-{}\".format(PATH)\n",
    "TEST_PATH = \"data/test-{}\".format(PATH)\n",
    "net = [128, 32, 64]\n",
    "lr = 3e-3\n",
    "lr = 1e-2\n",
    "batch_size = 50\n",
    "num_steps = 5500\n",
    "\n",
    "# With gaussian weights.\\\n",
    "PATH = \"5rel-10numAttrs-22totalAttrs-100maxTblSize-10buckets-3000.csv\"\n",
    "NUM_FIELDS = 34\n",
    "TRAIN_PATH = \"data/train-{}\".format(PATH)\n",
    "TEST_PATH = \"data/test-{}\".format(PATH)\n",
    "net = [64,32]; lr = 1e-2\n",
    "net = [64, 16, 64]; lr = 5e-3\n",
    "net = [64]; lr = 1e-3\n",
    "batch_size = 128\n",
    "num_steps = 5500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.float32)\n",
      "(TensorShape([Dimension(None), Dimension(33)]), TensorShape([Dimension(None)]))\n",
      "(tf.float32, tf.float32)\n",
      "(TensorShape([Dimension(None), Dimension(33)]), TensorShape([Dimension(None)]))\n"
     ]
    }
   ],
   "source": [
    "def DatasetFromPath(pathname):\n",
    "    dataset = tf.data.TextLineDataset(pathname)\n",
    "\n",
    "    def ParseLine(line):\n",
    "        fields = tf.decode_csv(line, [[0.0]] * NUM_FIELDS)\n",
    "        # Last number is label.\n",
    "        return tf.reshape(tf.stack(fields[:-1]), [-1]), fields[-1]\n",
    "\n",
    "    def MeanCenterBatch(feat, lab):\n",
    "        feat_mean, feat_var = tf.nn.moments(feat, axes=[0])\n",
    "        lab_mean, lab_var = tf.nn.moments(lab, axes=[0])\n",
    "        feat = (feat - feat_mean) / (feat_var + 1e-4)\n",
    "        lab = (lab - lab_mean) / (lab_var + 1e-4)\n",
    "        return feat, lab\n",
    "    \n",
    "    dataset = dataset.map(ParseLine)\n",
    "    # dataset = dataset.shuffle(2000).repeat().batch(batch_size)\n",
    "#     dataset = dataset.shuffle_and_repeat(20000).batch(batch_size)\n",
    "    dataset = dataset.shuffle(20000, reshuffle_each_iteration=True) \\\n",
    "        .batch(batch_size) \\\n",
    "        .repeat()\n",
    "#         .map(MeanCenterBatch) \\\n",
    "\n",
    "    print(dataset.output_types)\n",
    "    print(dataset.output_shapes)\n",
    "    return dataset\n",
    "\n",
    "# Train.\n",
    "dataset = DatasetFromPath(TRAIN_PATH)\n",
    "iterator = tf.data.Iterator.from_structure(\n",
    "    dataset.output_types, dataset.output_shapes)\n",
    "iterator_init = iterator.make_initializer(dataset)\n",
    "# Test.\n",
    "test_dataset = DatasetFromPath(TEST_PATH)\n",
    "test_iterator_init = iterator.make_initializer(test_dataset)\n",
    "# Shared.\n",
    "feature_batch, label_batch = iterator.get_next()\n",
    "\n",
    "    \n",
    "\n",
    "# mean, var = tf.nn.moments(feature_batch, axes=[0])\n",
    "# feature_batch = (feature_batch - mean) / tf.sqrt(var)\n",
    "# mean, var = tf.nn.moments(label_batch, axes=[0])\n",
    "# label_batch = (label_batch - mean) / var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Train Loss is illegal; using Train_Loss instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Train_Loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Graph definition.\n",
    "for i in range(len(net)):\n",
    "    dense = tf.layers.dense(\n",
    "        dense if i > 0 else feature_batch,\n",
    "        net[i],\n",
    "        kernel_initializer=tf.truncated_normal_initializer,\n",
    "#         kernel_initializer=tf.initializers.variance_scaling(),\n",
    "        activation=tf.nn.relu,\n",
    "#         activation=tf.nn.tanh,\n",
    "    )\n",
    "out = tf.layers.dense(\n",
    "    dense,\n",
    "    1,\n",
    "        kernel_initializer=tf.truncated_normal_initializer,\n",
    "#         kernel_initializer=tf.initializers.variance_scaling(),\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(out - label_batch))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "train_step = optimizer.minimize(\n",
    "    loss, global_step=global_step)\n",
    "\n",
    "tf.summary.scalar('Train Loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step 1 , loss 57772.86 , avg test loss 57732.234\n",
      " true labels [ 2.  1.  1. 13.  8.  1. 14.  8. 10.  9.  8.  1.  1.  4.  1. 13.  1.  8.\n",
      " 11. 13.]\n",
      " pred labels [185.3111   115.60453  408.39185  166.36569  124.35801  361.5647\n",
      " 109.806526 216.58649  185.28848  320.46982  177.49396   96.40482\n",
      " 226.3877   122.84504  138.63852  160.12936  118.6876   448.54288\n",
      " 101.84559  131.57129 ]\n",
      "global_step 101 , loss 681.41187 , avg test loss 29244.17\n",
      "global_step 201 , loss 230.93488 , avg test loss 19611.63\n",
      "global_step 301 , loss 238.6412 , avg test loss 14769.838\n",
      "global_step 401 , loss 135.23615 , avg test loss 11843.418\n",
      "global_step 501 , loss 160.80075 , avg test loss 9896.5205\n",
      "global_step 601 , loss 222.72032 , avg test loss 8506.685\n",
      "global_step 701 , loss 180.89676 , avg test loss 7467.824\n",
      "global_step 801 , loss 239.87991 , avg test loss 6652.2207\n",
      "global_step 901 , loss 211.84882 , avg test loss 5992.159\n",
      "global_step 1001 , loss 53.047638 , avg test loss 5466.6055\n",
      " true labels [ 1.  1. 15.  1.  2. 12.  1.  1.  3.  1. 13.  3.  1. 15. 12.  1. 11.  4.\n",
      " 11. 13.]\n",
      " pred labels [-2.0621047  4.5995927  3.189732   3.3648114 10.533223   6.433888\n",
      "  5.0080457 10.801381   7.602711  10.300801   4.1460156  5.780171\n",
      "  7.127461   3.189732   3.460133  -3.1638408  9.976865   8.468862\n",
      "  4.657582   3.6038861]\n",
      "global_step 1101 , loss 94.5697 , avg test loss 5014.277\n",
      "global_step 1201 , loss 47.728363 , avg test loss 4640.537\n",
      "global_step 1301 , loss 147.09718 , avg test loss 4316.3276\n",
      "global_step 1401 , loss 98.58536 , avg test loss 4031.286\n",
      "global_step 1501 , loss 88.21372 , avg test loss 3786.2136\n",
      "global_step 1601 , loss 33.13723 , avg test loss 3579.6602\n",
      "global_step 1701 , loss 30.665997 , avg test loss 3384.9575\n",
      "global_step 1801 , loss 153.78394 , avg test loss 3209.1074\n",
      "global_step 1901 , loss 152.35202 , avg test loss 3056.2422\n",
      "global_step 2001 , loss 32.272385 , avg test loss 2914.8472\n",
      " true labels [ 1.  1.  3.  1.  7. 13.  1. 13.  1.  1.  1.  1.  1.  1.  1.  4.  1. 11.\n",
      " 13.  1.]\n",
      " pred labels [ 1.6788088  3.3734787  6.868994   7.6320558 10.549224   6.0950685\n",
      "  7.3591485  7.7760377  6.6278443  2.7787616  6.3227296 11.762717\n",
      "  8.828452   6.296515   5.6501985  6.874945   3.9186924  4.8318696\n",
      "  5.62139    3.430518 ]\n",
      "global_step 2101 , loss 97.645996 , avg test loss 2786.1934\n",
      "global_step 2201 , loss 197.73863 , avg test loss 2669.2412\n",
      "global_step 2301 , loss 140.13432 , avg test loss 2566.5413\n",
      "global_step 2401 , loss 98.18724 , avg test loss 2467.3157\n",
      "global_step 2501 , loss 143.20749 , avg test loss 2378.6611\n",
      "global_step 2601 , loss 95.45386 , avg test loss 2295.7554\n",
      "global_step 2701 , loss 163.8443 , avg test loss 2216.7744\n",
      "global_step 2801 , loss 203.47614 , avg test loss 2147.9175\n",
      "global_step 2901 , loss 84.01992 , avg test loss 2077.3064\n",
      "global_step 3001 , loss 140.45502 , avg test loss 2011.331\n",
      " true labels [ 1.  7.  1.  1. 16. 11.  1.  1.  1.  1.  3.  1.  1.  1.  4.  1.  7.  1.\n",
      " 16.  1.]\n",
      " pred labels [ 7.1997743 10.160171   7.600058   6.4586244  9.014487  10.832679\n",
      "  6.6336236  6.336432   5.9733796  7.675917   8.899276   7.192984\n",
      "  7.8345017  6.5615296  6.735831   4.215628  11.74108    6.5642304\n",
      "  6.862265   7.2175813]\n",
      "global_step 3101 , loss 85.712204 , avg test loss 1949.1871\n",
      "global_step 3201 , loss 151.80273 , avg test loss 1892.5687\n",
      "global_step 3301 , loss 139.69347 , avg test loss 1840.2424\n",
      "global_step 3401 , loss 38.202423 , avg test loss 1790.354\n",
      "global_step 3501 , loss 194.61192 , avg test loss 1741.4855\n",
      "global_step 3601 , loss 84.18028 , avg test loss 1700.2223\n",
      "global_step 3701 , loss 147.58386 , avg test loss 1662.0385\n",
      "global_step 3801 , loss 83.6573 , avg test loss 1623.7905\n",
      "global_step 3901 , loss 83.28106 , avg test loss 1585.8654\n",
      "global_step 4001 , loss 84.14477 , avg test loss 146.29362\n",
      " true labels [ 6. 12. 12.  4. 12.  1.  1. 11.  1. 91.  1.  1.  1. 12.  1.  9.  4.  1.\n",
      "  1. 15.]\n",
      " pred labels [6.1385016 6.4611945 5.860105  6.1840186 5.885923  6.2658973 6.328916\n",
      " 6.2983985 6.161611  6.862043  6.1850142 6.8771186 5.9919887 5.815435\n",
      " 5.9624    6.4129615 6.1808753 5.3448825 5.962934  5.647825 ]\n",
      "global_step 4101 , loss 92.70865 , avg test loss 129.43518\n",
      "global_step 4201 , loss 149.8487 , avg test loss 127.22302\n",
      "global_step 4301 , loss 27.693584 , avg test loss 123.72715\n",
      "global_step 4401 , loss 206.39505 , avg test loss 123.48307\n",
      "global_step 4501 , loss 88.1912 , avg test loss 123.44399\n",
      "global_step 4601 , loss 151.4547 , avg test loss 119.889015\n",
      "global_step 4701 , loss 103.89495 , avg test loss 117.65857\n",
      "global_step 4801 , loss 140.81993 , avg test loss 116.53865\n",
      "global_step 4901 , loss 84.15978 , avg test loss 117.849915\n",
      "global_step 5001 , loss 81.955505 , avg test loss 115.66994\n",
      " true labels [ 1.  2.  3. 14.  1.  4.  7.  9.  1.  8. 12.  1.  1.  3. 11.  1. 13.  1.\n",
      "  1.  1.]\n",
      " pred labels [4.0346255 5.8331637 5.8336825 4.662441  5.8335605 5.6641574 7.5120654\n",
      " 4.9179416 5.2317557 5.850803  5.6786227 7.950351  5.5831027 5.7783237\n",
      " 5.534473  3.1221192 4.5378146 5.971721  5.7675204 5.7557406]\n",
      "global_step 5101 , loss 97.44343 , avg test loss 116.82678\n",
      "global_step 5201 , loss 31.98935 , avg test loss 116.396255\n",
      "global_step 5301 , loss 85.22108 , avg test loss 114.73582\n",
      "global_step 5401 , loss 81.37941 , avg test loss 115.220116\n",
      "global_step 5501 , loss 84.38861 , avg test loss 114.492516\n",
      "global_step 5601 , loss 139.12177 , avg test loss 108.24919\n",
      "global_step 5701 , loss 47.553463 , avg test loss 109.65938\n",
      "global_step 5801 , loss 145.36304 , avg test loss 111.18081\n",
      "global_step 5901 , loss 211.26134 , avg test loss 110.02754\n",
      "global_step 6001 , loss 137.84113 , avg test loss 109.06737\n",
      " true labels [ 1.  1.  9.  7.  1.  8.  3.  2.  1.  1.  4.  2.  1.  3. 13.  1. 13. 16.\n",
      "  4.  1.]\n",
      " pred labels [6.802846  6.593236  6.5386095 6.9323015 6.471593  6.675084  6.5243273\n",
      " 6.5186815 6.4370394 5.557103  6.5065355 6.675008  6.5461473 6.5293627\n",
      " 6.7455645 6.4518213 6.6359453 6.803479  6.5271044 6.5134325]\n",
      "global_step 6101 , loss 84.41319 , avg test loss 110.94448\n",
      "global_step 6201 , loss 105.89892 , avg test loss 109.988625\n",
      "global_step 6301 , loss 85.88165 , avg test loss 106.15759\n",
      "global_step 6401 , loss 84.04266 , avg test loss 106.62715\n",
      "global_step 6501 , loss 39.672695 , avg test loss 107.47001\n",
      "global_step 6601 , loss 36.00051 , avg test loss 107.41659\n",
      "global_step 6701 , loss 38.796707 , avg test loss 107.28783\n",
      "global_step 6801 , loss 140.81468 , avg test loss 105.340775\n",
      "global_step 6901 , loss 104.74429 , avg test loss 107.27937\n",
      "global_step 7001 , loss 83.57265 , avg test loss 107.114136\n",
      " true labels [ 1.  1.  1.  1.  9. 12.  1.  1. 12.  8.  1. 11. 10.  6.  2.  1.  1.  3.\n",
      " 14. 13.]\n",
      " pred labels [5.647314  5.735285  5.7434635 5.54838   5.6587467 5.9051304 5.8644047\n",
      " 5.8458805 5.9113865 5.201288  5.73736   5.8789005 5.641569  6.2339115\n",
      " 5.8524723 5.9228153 5.4728165 5.844385  5.9642277 6.26272  ]\n",
      "global_step 7101 , loss 26.223562 , avg test loss 108.65558\n",
      "global_step 7201 , loss 80.94295 , avg test loss 109.20239\n",
      "global_step 7301 , loss 254.03256 , avg test loss 110.00667\n",
      "global_step 7401 , loss 35.398624 , avg test loss 108.53536\n",
      "global_step 7501 , loss 90.02611 , avg test loss 110.86206\n",
      "global_step 7601 , loss 147.95377 , avg test loss 106.803635\n",
      "global_step 7701 , loss 218.17429 , avg test loss 102.01144\n",
      "global_step 7801 , loss 94.77043 , avg test loss 99.7575\n",
      "global_step 7901 , loss 27.79 , avg test loss 99.08721\n",
      "global_step 8001 , loss 83.36378 , avg test loss 96.02251\n",
      " true labels [ 1.  1.  4.  1.  2.  1. 15.  3.  1. 12.  2.  6.  2.  1.  1. 10. 15. 11.\n",
      "  8. 12.]\n",
      " pred labels [4.2442827 4.0809145 4.4230394 4.2513323 4.158284  4.100728  4.3103533\n",
      " 3.334851  3.164708  2.2214973 4.483617  5.555455  4.5529833 4.4079485\n",
      " 4.4909716 3.368611  4.1408587 3.1363952 4.322034  3.2309806]\n",
      "global_step 8101 , loss 140.3812 , avg test loss 97.965065\n",
      "global_step 8201 , loss 40.184597 , avg test loss 94.90293\n",
      "global_step 8301 , loss 32.328568 , avg test loss 95.105064\n",
      "global_step 8401 , loss 139.58932 , avg test loss 93.12211\n",
      "global_step 8501 , loss 91.745636 , avg test loss 89.62962\n",
      "global_step 8601 , loss 83.335106 , avg test loss 93.155106\n",
      "global_step 8701 , loss 92.49761 , avg test loss 92.50505\n",
      "global_step 8801 , loss 149.94995 , avg test loss 92.70125\n",
      "global_step 8901 , loss 101.65056 , avg test loss 95.190056\n",
      "global_step 9001 , loss 139.19797 , avg test loss 95.55918\n",
      " true labels [ 1.  1.  1. 10.  8.  1.  1.  1. 14.  4.  1. 12.  1.  1.  8.  1.  2.  2.\n",
      "  2.  1.]\n",
      " pred labels [5.528711  5.8474975 5.266954  5.2696166 6.1594634 5.5302672 5.3517165\n",
      " 5.6128936 5.553308  5.5365844 5.2717586 5.5091186 5.2814937 5.550592\n",
      " 6.118219  5.4835143 5.3906493 5.5024657 5.395227  5.3997436]\n",
      "global_step 9101 , loss 79.704605 , avg test loss 95.4426\n",
      "global_step 9201 , loss 24.43472 , avg test loss 96.3503\n",
      "global_step 9301 , loss 83.43373 , avg test loss 100.94535\n",
      "global_step 9401 , loss 146.35559 , avg test loss 101.76328\n",
      "global_step 9501 , loss 150.97119 , avg test loss 102.48872\n",
      "global_step 9601 , loss 82.88725 , avg test loss 103.89299\n",
      "global_step 9701 , loss 203.22841 , avg test loss 102.550735\n",
      "global_step 9801 , loss 148.76984 , avg test loss 100.820854\n",
      "global_step 9901 , loss 47.182545 , avg test loss 102.26334\n",
      "global_step 10001 , loss 95.09298 , avg test loss 102.31267\n",
      " true labels [ 3. 13.  1. 11.  2. 11. 11.  1.  1.  3.  1. 12.  1.  3.  1.  1.  1. 15.\n",
      " 11.  1.]\n",
      " pred labels [7.085011  7.1760144 7.1698194 7.4774957 7.0706983 7.4490476 7.540282\n",
      " 7.3665814 7.2182508 7.045857  7.059956  7.1861157 6.8558545 7.085011\n",
      " 7.0369153 7.1760755 7.7317853 7.272282  6.5576367 7.0742993]\n",
      "global_step 10101 , loss 93.72052 , avg test loss 100.35602\n"
     ]
    }
   ],
   "source": [
    "# Train.\n",
    "sess_args = {\n",
    "             'checkpoint_dir': os.getcwd(),\n",
    "#              'save_checkpoint_secs': 60, \n",
    "             'save_summaries_secs': 60,\n",
    "             'log_step_count_steps': 5000,\n",
    "             'hooks': [tf.train.NanTensorHook(loss),\n",
    "                       tf.train.StopAtStepHook(last_step=10000),\n",
    "#                       tf.train.LoggingTensorHook([global_step, loss], every_n_iter=500)\n",
    "                      ],\n",
    "            }\n",
    "\n",
    "loss_vals = collections.deque(maxlen=40)\n",
    "i = 0\n",
    "# with tf.train.MonitoredTrainingSession(**sess_args) as sess:\n",
    "#     while not sess.should_stop():\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    running_test_loss = 0.0\n",
    "    while True:\n",
    "        sess.run(iterator_init)\n",
    "        _, loss_val, global_step_val = sess.run([train_step, loss, global_step])\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            sess.run(test_iterator_init)\n",
    "            test_loss_val, actual_labels, predicted_labels = sess.run([loss, label_batch, out])\n",
    "\n",
    "            loss_vals.append(test_loss_val)\n",
    "            avg_test_loss = np.mean(loss_vals)\n",
    "            print('global_step', global_step_val, ', loss', loss_val, ', avg test loss', avg_test_loss)\n",
    "            if i % 1000 == 0:\n",
    "                print(' true labels', actual_labels[:20])\n",
    "                print(' pred labels', predicted_labels[:20].reshape(-1,))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
