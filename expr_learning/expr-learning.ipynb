{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zongheng/anaconda/envs/ray-0321/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "# TODO(zongheng): visualization via tensorboard.\n",
    "# TODO(zongheng): print some prediction from test set\n",
    "\n",
    "# With gaussian weights.\n",
    "# 500 training pts; 1000 testing.\n",
    "PATH = \"5rel-10numAttrs-22totalAttrs-100maxTblSize-10buckets-3000.csv\"\n",
    "NUM_FIELDS = 34\n",
    "TRAIN_PATH = \"data/train-{}\".format(PATH)\n",
    "TEST_PATH = \"data/test-{}\".format(PATH)\n",
    "net = [64,32]; lr = 1e-2\n",
    "net = [64, 16, 64]; lr = 5e-3\n",
    "net = [64,64]; lr =  1e-3\n",
    "batch_size = 500\n",
    "num_steps = 5500\n",
    "\n",
    "# With random op types.\n",
    "PATH = \"5rel-10numAttrs-28totalAttrs-100maxTblSize-10buckets-3000-1526330982.csv\"\n",
    "NUM_FIELDS = 40\n",
    "TRAIN_PATH = \"data/train-{}\".format(PATH)\n",
    "TEST_PATH = \"data/test-{}\".format(PATH)\n",
    "net = [128,128]; lr =  1e-3\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.float32)\n",
      "(TensorShape([Dimension(None), Dimension(39)]), TensorShape([Dimension(None)]))\n",
      "(tf.float32, tf.float32)\n",
      "(TensorShape([Dimension(None), Dimension(39)]), TensorShape([Dimension(None)]))\n"
     ]
    }
   ],
   "source": [
    "def DatasetFromPath(pathname):\n",
    "    dataset = tf.data.TextLineDataset(pathname)\n",
    "\n",
    "    def ParseLine(line):\n",
    "        fields = tf.decode_csv(line, [[0.0]] * NUM_FIELDS)\n",
    "        # Last number is label.\n",
    "        return tf.reshape(tf.stack(fields[:-1]), [-1]), fields[-1]\n",
    "\n",
    "    def MeanCenterBatch(feat, lab):\n",
    "        feat_mean, feat_var = tf.nn.moments(feat, axes=[0])\n",
    "        lab_mean, lab_var = tf.nn.moments(lab, axes=[0])\n",
    "        feat = (feat - feat_mean) / (tf.sqrt(feat_var) + 1e-8)\n",
    "        lab = (lab - lab_mean) / (tf.sqrt(lab_var) + 1e-8)\n",
    "        return feat, lab\n",
    "    \n",
    "#     # K-point dataset, for sanity checking.\n",
    "#     K = 10\n",
    "#     dataset = dataset.take(K).map(ParseLine)\n",
    "#     dataset = dataset.shuffle(20000, reshuffle_each_iteration=False) \\\n",
    "#         .batch(K).map(MeanCenterBatch) \\\n",
    "#         .repeat()\n",
    "\n",
    "    dataset = dataset.cache().map(ParseLine)\n",
    "    dataset = dataset.shuffle(20000, reshuffle_each_iteration=True) \\\n",
    "        .repeat() \\\n",
    "        .batch(batch_size) #\\\n",
    "#         .map(MeanCenterBatch) \n",
    "\n",
    "    print(dataset.output_types)\n",
    "    print(dataset.output_shapes)\n",
    "    return dataset\n",
    "\n",
    "# Train.\n",
    "dataset = DatasetFromPath(TRAIN_PATH)\n",
    "iterator = tf.data.Iterator.from_structure(\n",
    "    dataset.output_types, dataset.output_shapes)\n",
    "iterator_init = iterator.make_initializer(dataset)\n",
    "# Test.\n",
    "test_dataset = DatasetFromPath(TEST_PATH)\n",
    "test_iterator_init = iterator.make_initializer(test_dataset)\n",
    "# Shared.\n",
    "feature_batch, label_batch = iterator.get_next()\n",
    "\n",
    "    \n",
    "\n",
    "# mean, var = tf.nn.moments(feature_batch, axes=[0])\n",
    "# feature_batch = (feature_batch - mean) / tf.sqrt(var)\n",
    "# mean, var = tf.nn.moments(label_batch, axes=[0])\n",
    "# label_batch = (label_batch - mean) / var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zongheng/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "out Tensor(\"dense_2/BiasAdd:0\", shape=(?, 1), dtype=float32) label_batch Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=float32)\n",
      "INFO:tensorflow:Summary name Train Loss is illegal; using Train_Loss instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0 is illegal; using dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0 is illegal; using dense/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0 is illegal; using dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0 is illegal; using dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0 is illegal; using dense_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0 is illegal; using dense_2/bias_0 instead.\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Graph definition.\n",
    "dense = feature_batch\n",
    "for i in range(len(net)):\n",
    "    dense = tf.layers.dense(\n",
    "        dense,\n",
    "        net[i],\n",
    "#         kernel_initializer=tf.truncated_normal_initializer(stddev=20.0),\n",
    "#         kernel_initializer=tf.initializers.variance_scaling(scale=10.0),\n",
    "        kernel_initializer=tf.initializers.variance_scaling(),\n",
    "#         kernel_initializer=tf.initializers.orthogonal(),\n",
    "#         kernel_initializer=tf.initializers.uniform_unit_scaling(),\n",
    "#         kernel_initializer=tf.initializers.random_normal(stddev=10),\n",
    "#         kernel_initializer=tf.initializers.random_uniform(-100, 100),\n",
    "#         kernel_initializer=tf.initializers.zeros(),\n",
    "#         bias_initializer=tf.initializers.zeros(),\n",
    "#         activation=tf.nn.sigmoid,\n",
    "        activation=tf.nn.relu,\n",
    "#         activation=tf.nn.tanh,\n",
    "    )\n",
    "out = tf.layers.dense(\n",
    "    dense,\n",
    "    1,\n",
    "#         kernel_initializer=tf.truncated_normal_initializer(stddev=20.0),\n",
    "#         kernel_initializer=tf.initializers.variance_scaling(scale=10.0),\n",
    "#         kernel_initializer=tf.initializers.orthogonal(),\n",
    "#         kernel_initializer=tf.initializers.uniform_unit_scaling(),\n",
    "#         kernel_initializer=tf.initializers.random_normal(stddev=10),\n",
    "        kernel_initializer=tf.initializers.variance_scaling(),\n",
    "#         kernel_initializer=tf.initializers.random_uniform(-100, 100),\n",
    "#         kernel_initializer=tf.initializers.zeros(),\n",
    "#         bias_initializer=tf.initializers.zeros(),\n",
    ")\n",
    "\n",
    "\n",
    "all_vars = tf.trainable_variables()\n",
    "l1_regularizer = tf.contrib.layers.l1_regularizer(scale=0.05)  # TODO: scale=?\n",
    "regularization_penalty = tf.contrib.layers.apply_regularization(\n",
    "    l1_regularizer, all_vars)\n",
    "\n",
    "# NOTE: critical to reshape / or keep the order correct...\n",
    "# out: [B, 1]\n",
    "# label_batch [B]\n",
    "# loss = tf.reduce_mean(tf.square(out - label_batch))\n",
    "# loss = tf.reduce_mean(tf.square(out - label_batch)) \n",
    "# loss = tf.reduce_mean(tf.square(tf.reshape(out,[-1]) - label_batch)) + regularization_penalty\n",
    "print('out', out, 'label_batch', label_batch)\n",
    "loss = tf.reduce_mean(tf.abs(tf.reshape(out,[-1]) - label_batch))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "train_step = optimizer.minimize(\n",
    "    loss, global_step=global_step)\n",
    "\n",
    "# For visualization.\n",
    "tf.summary.scalar('Train Loss', loss)\n",
    "for v in all_vars:\n",
    "    tf.summary.histogram(v.name, v)\n",
    "all_vars_norms = [tf.norm(v) for v in all_vars]\n",
    "print(len(all_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "global_step 1 , loss 27.93195\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   7.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   1.  16.]]\n",
      " true labels train [ 17.   1. 100.   1. 100.  10.   1.   1.  48.   1.]\n",
      " pred labels train [-4.604067  -4.5909424 -4.180997  -4.5274982 -4.5368233 -4.4870157\n",
      " -4.468717  -4.2207484 -4.159872  -4.3090906]\n",
      " all vars norms [10.09834, 0.009213345, 10.02359, 0.008062058, 0.8160549, 0.0009999997]\n",
      "global_step 101 , loss 20.046059\n",
      "global_step 201 , loss 16.755241\n",
      "global_step 301 , loss 16.984032\n",
      "global_step 401 , loss 14.691135\n",
      "global_step 501 , loss 12.768385\n",
      "global_step 601 , loss 12.917978\n",
      "global_step 701 , loss 14.443335\n",
      "global_step 801 , loss 12.460282\n",
      "global_step 901 , loss 12.20053\n",
      "global_step 1001 , loss 11.053185\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  84.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  65.]]\n",
      " true labels train [100.  10.  10.  59.  91.   1.  10.  10.  35.   1.]\n",
      " pred labels train [24.416016  11.422095   2.069821  55.55824   60.049015   8.738217\n",
      " 10.246307   2.132547   9.500415  -1.2695255]\n",
      " all vars norms [26.28979, 0.12981233, 13.412973, 0.069956146, 1.3709964, 0.0075081848]\n",
      "global_step 1101 , loss 11.521091\n",
      "global_step 1201 , loss 11.146463\n",
      "global_step 1301 , loss 8.880022\n",
      "global_step 1401 , loss 7.5454035\n",
      "global_step 1501 , loss 7.692953\n",
      "global_step 1601 , loss 7.283136\n",
      "global_step 1701 , loss 6.337735\n",
      "global_step 1801 , loss 6.544182\n",
      "global_step 1901 , loss 6.4411364\n",
      "global_step 2001 , loss 5.3801265\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.   0.   1.   0.  72.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   3.]]\n",
      " true labels train [ 82.   8.  10. 100. 100.  10.  10.  10.   1.  39.]\n",
      " pred labels train [83.50144    8.893285  15.716277  98.05931   63.03091   10.311919\n",
      " 11.262212  10.056361   1.3674202 42.786198 ]\n",
      " all vars norms [30.31787, 0.17094985, 20.225716, 0.0823331, 1.987108, 0.0097337235]\n",
      "global_step 2101 , loss 4.7247376\n",
      "global_step 2201 , loss 4.5293345\n",
      "global_step 2301 , loss 5.4139686\n",
      "global_step 2401 , loss 4.462475\n",
      "global_step 2501 , loss 4.6988916\n",
      "global_step 2601 , loss 3.6966836\n",
      "global_step 2701 , loss 4.5048876\n",
      "global_step 2801 , loss 4.039928\n",
      "global_step 2901 , loss 3.7544298\n",
      "global_step 3001 , loss 3.5760908\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  44.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.  73.]]\n",
      " true labels train [10.  1. 15.  1. 10. 33.  1. 10. 17.  2.]\n",
      " pred labels train [ 8.649428    0.82215154 15.14882    -0.20113148  6.3025756  27.969965\n",
      "  0.82244146  4.3167214  40.23728     1.3257707 ]\n",
      " all vars norms [34.207657, 0.20988165, 21.935665, 0.09683189, 2.1963005, 0.0115043195]\n",
      "global_step 3101 , loss 3.3147233\n",
      "global_step 3201 , loss 4.304139\n",
      "global_step 3301 , loss 4.42305\n",
      "global_step 3401 , loss 2.927472\n",
      "global_step 3501 , loss 2.588766\n",
      "global_step 3601 , loss 3.663519\n",
      "global_step 3701 , loss 3.3460078\n",
      "global_step 3801 , loss 2.8545876\n",
      "global_step 3901 , loss 2.5923078\n",
      "global_step 4001 , loss 3.3338358\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   8.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  87.]]\n",
      " true labels train [ 4. 10.  1.  1. 10.  2. 10.  5. 48.  1.]\n",
      " pred labels train [ 4.82183   12.293686   2.8581498  2.4943163 12.48719    4.18334\n",
      " 11.78415    5.1473246 47.024403   2.3526738]\n",
      " all vars norms [37.37887, 0.23781687, 22.503527, 0.117569126, 2.3024142, 0.013512542]\n",
      "global_step 4101 , loss 4.0223045\n",
      "global_step 4201 , loss 3.0596423\n",
      "global_step 4301 , loss 2.6877246\n",
      "global_step 4401 , loss 2.9896321\n",
      "global_step 4501 , loss 2.4928007\n",
      "global_step 4601 , loss 2.9654853\n",
      "global_step 4701 , loss 3.5310366\n",
      "global_step 4801 , loss 2.2795904\n",
      "global_step 4901 , loss 2.5747285\n",
      "global_step 5001 , loss 2.7712526\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   5.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  70.]]\n",
      " true labels train [10. 10. 32. 18.  1. 10. 10.  5.  1. 10.]\n",
      " pred labels train [12.581464    9.213495   31.408098   19.44119     0.47066885 12.095766\n",
      "  9.100489    4.6030293  -0.15999025  9.520297  ]\n",
      " all vars norms [39.623436, 0.25864136, 22.802387, 0.13114439, 2.3579402, 0.014786784]\n",
      "global_step 5101 , loss 3.9269567\n",
      "global_step 5201 , loss 2.351598\n",
      "global_step 5301 , loss 2.3700094\n",
      "global_step 5401 , loss 3.0562067\n",
      "global_step 5501 , loss 3.0321693\n",
      "global_step 5601 , loss 2.0822246\n",
      "global_step 5701 , loss 2.8389351\n",
      "global_step 5801 , loss 2.2771811\n",
      "global_step 5901 , loss 2.4844217\n",
      "global_step 6001 , loss 2.5805058\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  43.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.  30.]]\n",
      " true labels train [  6.  10.  10. 100. 100.  53.  10.   1.   4.  10.]\n",
      " pred labels train [  4.2817545    7.67183     11.669887   100.013535    97.82849\n",
      "  60.199196     9.609408     0.15025726   2.9032128   10.268575  ]\n",
      " all vars norms [41.508335, 0.27579817, 23.026047, 0.14015357, 2.394076, 0.0160285]\n",
      "global_step 6101 , loss 2.3213716\n",
      "global_step 6201 , loss 2.6202135\n",
      "global_step 6301 , loss 2.646184\n",
      "global_step 6401 , loss 2.4436083\n",
      "global_step 6501 , loss 2.2048898\n",
      "global_step 6601 , loss 2.3421965\n",
      "global_step 6701 , loss 2.9920106\n",
      "global_step 6801 , loss 2.4081366\n",
      "global_step 6901 , loss 2.6662612\n",
      "global_step 7001 , loss 2.0518532\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  26.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  47.]]\n",
      " true labels train [  1.  10.   1.  91.  10.  11. 100.   7.  21.  81.]\n",
      " pred labels train [ 0.7903105   9.590777   -0.64841783 88.41572    10.388465   10.498595\n",
      " 99.69229     7.6455894  17.880045   80.32615   ]\n",
      " all vars norms [43.111263, 0.2933134, 23.258778, 0.14497873, 2.4253259, 0.017160915]\n",
      "global_step 7101 , loss 2.1366944\n",
      "global_step 7201 , loss 3.0831327\n",
      "global_step 7301 , loss 3.073894\n",
      "global_step 7401 , loss 1.6438441\n",
      "global_step 7501 , loss 1.7302341\n",
      "global_step 7601 , loss 2.0426922\n",
      "global_step 7701 , loss 2.3658123\n",
      "global_step 7801 , loss 2.4169827\n",
      "global_step 7901 , loss 3.5330958\n",
      "global_step 8001 , loss 2.0327857\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.  24.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   1.  47.]]\n",
      " true labels train [ 1.  1.  1.  1. 10. 11. 91.  1.  1. 30.]\n",
      " pred labels train [ 0.7223183   2.413261    2.4650683   0.36908683  9.152632    9.359217\n",
      " 90.435646   -5.03851     0.5768639  22.74107   ]\n",
      " all vars norms [44.475525, 0.31039774, 23.472706, 0.15319553, 2.4535408, 0.018122746]\n",
      "global_step 8101 , loss 2.1039934\n",
      "global_step 8201 , loss 2.0156176\n",
      "global_step 8301 , loss 2.3495226\n",
      "global_step 8401 , loss 2.1938248\n",
      "global_step 8501 , loss 2.2353458\n",
      "global_step 8601 , loss 1.8971429\n",
      "global_step 8701 , loss 1.8000621\n",
      "global_step 8801 , loss 1.7878594\n",
      "global_step 8901 , loss 2.0992832\n",
      "global_step 9001 , loss 2.108787\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   1.   0.   0.  55.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.  10.]]\n",
      " true labels train [17. 10.  1. 10. 32. 11.  1. 10. 10.  5.]\n",
      " pred labels train [37.959343   9.923217   1.495376   9.65214   28.154919  11.648368\n",
      " -4.2808557  9.594188   9.779714   2.450063 ]\n",
      " all vars norms [45.682228, 0.32340658, 23.761538, 0.15670003, 2.4969997, 0.019122796]\n",
      "global_step 9101 , loss 2.8011518\n",
      "global_step 9201 , loss 2.226576\n",
      "global_step 9301 , loss 3.2621555\n",
      "global_step 9401 , loss 1.7281088\n",
      "global_step 9501 , loss 2.3862388\n",
      "global_step 9601 , loss 2.9744554\n",
      "global_step 9701 , loss 2.4782667\n",
      "global_step 9801 , loss 2.1249804\n",
      "global_step 9901 , loss 1.9175708\n"
     ]
    }
   ],
   "source": [
    "# Train.\n",
    "sess_args = {\n",
    "#              'checkpoint_dir': os.getcwd(),\n",
    "#              'save_checkpoint_secs': 60, \n",
    "             'save_summaries_secs': 1,\n",
    "             'log_step_count_steps': 5000,\n",
    "             'hooks': [tf.train.NanTensorHook(loss),\n",
    "                       tf.train.StopAtStepHook(last_step=2000000),\n",
    "#                       tf.train.LoggingTensorHook([global_step, loss], every_n_iter=500)\n",
    "                      ],\n",
    "            }\n",
    "\n",
    "loss_vals = collections.deque(maxlen=40)\n",
    "i = 0\n",
    "with tf.train.MonitoredTrainingSession(**sess_args) as sess:\n",
    "    sess.run(iterator_init)\n",
    "    while not sess.should_stop():\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     running_test_loss = 0.0\n",
    "#     while True:\n",
    "        _, loss_val, global_step_val, true_feat_train, true_labels_train, pred_labels_train = sess.run(\n",
    "            [train_step, loss, global_step,  feature_batch, label_batch, out])\n",
    "\n",
    "        if i % 100 == 0:\n",
    "#             sess.run(test_iterator_init)\n",
    "#             test_loss_val, actual_labels, predicted_labels = sess.run([loss, label_batch, out])\n",
    "\n",
    "#             loss_vals.append(test_loss_val)\n",
    "#             avg_test_loss = np.mean(loss_vals)\n",
    "#             print('global_step', global_step_val, ', loss', loss_val, ', avg test loss', avg_test_loss)\n",
    "            print('global_step', global_step_val, ', loss', loss_val)\n",
    "            if i % 1000 == 0:\n",
    "#                 print(' true labels', actual_labels[:20].reshape(-1,))\n",
    "#                 print(' pred labels', predicted_labels[:20].reshape(-1,))\n",
    "                print(' true feats train', true_feat_train[:2])\n",
    "                print(' true labels train', true_labels_train[:10].reshape(-1,))\n",
    "                print(' pred labels train', pred_labels_train[:10].reshape(-1,))\n",
    "                print(' all vars norms', sess.run(all_vars_norms))\n",
    "        i += 1\n",
    "#         if i >= 10000:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
