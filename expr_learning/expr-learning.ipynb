{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zongheng/anaconda/envs/ray-0321/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "# TODO(zongheng): visualization via tensorboard.\n",
    "# TODO(zongheng): print some prediction from test set\n",
    "\n",
    "# With gaussian weights.\n",
    "# 500 training pts; 1000 testing.\n",
    "PATH = \"5rel-10numAttrs-22totalAttrs-100maxTblSize-10buckets-3000.csv\"\n",
    "NUM_FIELDS = 34\n",
    "TRAIN_PATH = \"data/train-{}\".format(PATH)\n",
    "TEST_PATH = \"data/test-{}\".format(PATH)\n",
    "net = [64,32]; lr = 1e-2\n",
    "net = [64, 16, 64]; lr = 5e-3\n",
    "net = [64,64]; lr =  1e-3\n",
    "batch_size = 500\n",
    "num_steps = 5500\n",
    "\n",
    "# With random op types.\n",
    "PATH = \"5rel-10numAttrs-28totalAttrs-100maxTblSize-10buckets-3000-1526330982.csv\"\n",
    "NUM_FIELDS = 40\n",
    "TRAIN_PATH = \"data/train-{}\".format(PATH)\n",
    "TEST_PATH = \"data/test-{}\".format(PATH)\n",
    "net = [128,128]; lr =  5e-3\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.float32)\n",
      "(TensorShape([Dimension(None), Dimension(39)]), TensorShape([Dimension(None)]))\n",
      "(tf.float32, tf.float32)\n",
      "(TensorShape([Dimension(None), Dimension(39)]), TensorShape([Dimension(None)]))\n"
     ]
    }
   ],
   "source": [
    "def DatasetFromPath(pathname):\n",
    "    dataset = tf.data.TextLineDataset(pathname)\n",
    "\n",
    "    def ParseLine(line):\n",
    "        fields = tf.decode_csv(line, [[0.0]] * NUM_FIELDS)\n",
    "        # Last number is label.\n",
    "        return tf.reshape(tf.stack(fields[:-1]), [-1]), fields[-1]\n",
    "\n",
    "    def MeanCenterBatch(feat, lab):\n",
    "        feat_mean, feat_var = tf.nn.moments(feat, axes=[0])\n",
    "        lab_mean, lab_var = tf.nn.moments(lab, axes=[0])\n",
    "        feat = (feat - feat_mean) / (tf.sqrt(feat_var) + 1e-8)\n",
    "        lab = (lab - lab_mean) / (tf.sqrt(lab_var) + 1e-8)\n",
    "        return feat, lab\n",
    "    \n",
    "#     # K-point dataset, for sanity checking.\n",
    "#     K = 10\n",
    "#     dataset = dataset.take(K).map(ParseLine)\n",
    "#     dataset = dataset.shuffle(20000, reshuffle_each_iteration=False) \\\n",
    "#         .batch(K).map(MeanCenterBatch) \\\n",
    "#         .repeat()\n",
    "\n",
    "    dataset = dataset.cache().map(ParseLine)\n",
    "    dataset = dataset.shuffle(20000, reshuffle_each_iteration=True) \\\n",
    "        .repeat() \\\n",
    "        .batch(batch_size) #\\\n",
    "#         .map(MeanCenterBatch) \n",
    "\n",
    "    print(dataset.output_types)\n",
    "    print(dataset.output_shapes)\n",
    "    return dataset\n",
    "\n",
    "# Train.\n",
    "dataset = DatasetFromPath(TRAIN_PATH)\n",
    "iterator = tf.data.Iterator.from_structure(\n",
    "    dataset.output_types, dataset.output_shapes)\n",
    "iterator_init = iterator.make_initializer(dataset)\n",
    "# Test.\n",
    "test_dataset = DatasetFromPath(TEST_PATH)\n",
    "test_iterator_init = iterator.make_initializer(test_dataset)\n",
    "# Shared.\n",
    "feature_batch, label_batch = iterator.get_next()\n",
    "\n",
    "    \n",
    "\n",
    "# mean, var = tf.nn.moments(feature_batch, axes=[0])\n",
    "# feature_batch = (feature_batch - mean) / tf.sqrt(var)\n",
    "# mean, var = tf.nn.moments(label_batch, axes=[0])\n",
    "# label_batch = (label_batch - mean) / var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zongheng/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "out Tensor(\"dense_2/BiasAdd:0\", shape=(?, 1), dtype=float32) label_batch Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=float32)\n",
      "INFO:tensorflow:Summary name Train Loss is illegal; using Train_Loss instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0 is illegal; using dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0 is illegal; using dense/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0 is illegal; using dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0 is illegal; using dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0 is illegal; using dense_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0 is illegal; using dense_2/bias_0 instead.\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Graph definition.\n",
    "dense = feature_batch\n",
    "for i in range(len(net)):\n",
    "    dense = tf.layers.dense(\n",
    "        dense,\n",
    "        net[i],\n",
    "#         kernel_initializer=tf.truncated_normal_initializer(stddev=20.0),\n",
    "#         kernel_initializer=tf.initializers.variance_scaling(scale=10.0),\n",
    "        kernel_initializer=tf.initializers.variance_scaling(),\n",
    "#         kernel_initializer=tf.initializers.orthogonal(),\n",
    "#         kernel_initializer=tf.initializers.uniform_unit_scaling(),\n",
    "#         kernel_initializer=tf.initializers.random_normal(stddev=10),\n",
    "#         kernel_initializer=tf.initializers.random_uniform(-100, 100),\n",
    "#         kernel_initializer=tf.initializers.zeros(),\n",
    "#         bias_initializer=tf.initializers.zeros(),\n",
    "#         activation=tf.nn.sigmoid,\n",
    "        activation=tf.nn.relu,\n",
    "#         activation=tf.nn.tanh,\n",
    "    )\n",
    "out = tf.layers.dense(\n",
    "    dense,\n",
    "    1,\n",
    "#         kernel_initializer=tf.truncated_normal_initializer(stddev=20.0),\n",
    "#         kernel_initializer=tf.initializers.variance_scaling(scale=10.0),\n",
    "#         kernel_initializer=tf.initializers.orthogonal(),\n",
    "#         kernel_initializer=tf.initializers.uniform_unit_scaling(),\n",
    "#         kernel_initializer=tf.initializers.random_normal(stddev=10),\n",
    "        kernel_initializer=tf.initializers.variance_scaling(),\n",
    "#         kernel_initializer=tf.initializers.random_uniform(-100, 100),\n",
    "#         kernel_initializer=tf.initializers.zeros(),\n",
    "#         bias_initializer=tf.initializers.zeros(),\n",
    ")\n",
    "\n",
    "\n",
    "all_vars = tf.trainable_variables()\n",
    "l1_regularizer = tf.contrib.layers.l1_regularizer(scale=0.0)  # TODO: scale=?\n",
    "regularization_penalty = tf.contrib.layers.apply_regularization(\n",
    "    l1_regularizer, all_vars)\n",
    "print('out', out, 'label_batch', label_batch)\n",
    "# NOTE: critical to reshape / or keep the order correct...\n",
    "# out: [B, 1]\n",
    "# label_batch [B]\n",
    "# loss = tf.reduce_mean(tf.square(out - label_batch))\n",
    "# loss = tf.reduce_mean(tf.square(out - label_batch)) \n",
    "loss = tf.reduce_mean(tf.abs(\n",
    "    tf.reshape(out,[-1]) - label_batch)) + regularization_penalty\n",
    "\n",
    "# loss = tf.reduce_mean(tf.abs(tf.reshape(out,[-1]) - label_batch))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "train_step = optimizer.minimize(\n",
    "    loss, global_step=global_step)\n",
    "\n",
    "# For visualization.\n",
    "tf.summary.scalar('Train Loss', loss)\n",
    "for v in all_vars:\n",
    "    tf.summary.histogram(v.name, v)\n",
    "all_vars_norms = [tf.norm(v) for v in all_vars]\n",
    "print(len(all_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "global_step 1 , loss 27.99059\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   1.   0.   0.   0.  96.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  36.]]\n",
      " true labels train [ 1.  1.  8. 10. 15.  1. 10.  8. 11. 17.]\n",
      " pred labels train [-5.848502  -4.2605777 -3.759995  -5.06429   -5.350711  -4.337441\n",
      " -3.9566202 -3.5314288 -5.9224596 -3.7954628]\n",
      " all vars norms [9.94791, 0.041827045, 9.899572, 0.044999097, 0.8957018, 0.004999998]\n",
      "global_step 101 , loss 19.476458\n",
      "global_step 201 , loss 14.603301\n",
      "global_step 301 , loss 9.9573765\n",
      "global_step 401 , loss 8.11655\n",
      "global_step 501 , loss 4.4928365\n",
      "global_step 601 , loss 4.2933807\n",
      "global_step 701 , loss 4.062057\n",
      "global_step 801 , loss 3.6786802\n",
      "global_step 901 , loss 2.542975\n",
      "global_step 1001 , loss 2.1147497\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  37.]]\n",
      " true labels train [10. 10. 10. 32.  1.  1.  1.  1. 94. 91.]\n",
      " pred labels train [10.333617   9.620668   9.691752  37.74938    0.584488   1.0748941\n",
      "  1.1494375  2.565316  27.655447  92.90839  ]\n",
      " all vars norms [53.0295, 0.54455954, 15.311475, 0.48938406, 2.5787976, 0.0155632505]\n",
      "global_step 1101 , loss 2.8538098\n",
      "global_step 1201 , loss 1.858765\n",
      "global_step 1301 , loss 2.1983354\n",
      "global_step 1401 , loss 3.056633\n",
      "global_step 1501 , loss 1.4342916\n",
      "global_step 1601 , loss 2.3185894\n",
      "global_step 1701 , loss 3.347297\n",
      "global_step 1801 , loss 2.3941336\n",
      "global_step 1901 , loss 3.2016988\n",
      "global_step 2001 , loss 1.5678402\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   1.   0.   0.  13.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   0.   0.   0.   0.   1.   0.  39.]]\n",
      " true labels train [100.  39.  10.   1.  10.  32.  10.   1.  48.  72.]\n",
      " pred labels train [1.0186786e+02 4.6813763e+01 9.9854364e+00 5.6977358e-02 9.8780327e+00\n",
      " 3.3541172e+01 9.8088665e+00 7.0023400e-01 4.8447353e+01 7.1994034e+01]\n",
      " all vars norms [60.099754, 0.5914165, 17.824955, 0.6458177, 2.7306015, 0.050994847]\n",
      "global_step 2101 , loss 1.5028763\n",
      "global_step 2201 , loss 1.9840298\n",
      "global_step 2301 , loss 1.7473111\n",
      "global_step 2401 , loss 2.665863\n",
      "global_step 2501 , loss 1.6152604\n",
      "global_step 2601 , loss 1.5088093\n",
      "global_step 2701 , loss 2.6766467\n",
      "global_step 2801 , loss 1.9158232\n",
      "global_step 2901 , loss 1.35531\n",
      "global_step 3001 , loss 1.6601782\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   1.   0.   0.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  31.]]\n",
      " true labels train [ 1.  1.  1.  1. 91.  1.  1. 73. 22.  1.]\n",
      " pred labels train [1.3363800e+00 5.2006461e-02 8.6505842e-01 6.1320710e-01 8.4261719e+01\n",
      " 1.3000014e+00 9.0174603e-01 7.2894615e+01 1.8384375e+01 7.9833579e-01]\n",
      " all vars norms [65.970924, 0.61284965, 20.429697, 0.78032446, 2.9248211, 0.077555954]\n",
      "global_step 3101 , loss 1.469246\n",
      "global_step 3201 , loss 1.9359595\n",
      "global_step 3301 , loss 1.4273293\n",
      "global_step 3401 , loss 1.7872512\n",
      "global_step 3501 , loss 1.3467159\n",
      "global_step 3601 , loss 1.1267583\n",
      "global_step 3701 , loss 1.353816\n",
      "global_step 3801 , loss 1.8769195\n",
      "global_step 3901 , loss 1.245713\n",
      "global_step 4001 , loss 1.1696739\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  36.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.  15.]]\n",
      " true labels train [ 1. 10.  1.  1. 10. 10.  1.  1.  1. 10.]\n",
      " pred labels train [1.1891538  9.811825   0.9251722  0.83090436 9.272789   9.242224\n",
      " 0.8995186  0.88193667 0.79320943 9.717055  ]\n",
      " all vars norms [72.37309, 0.63505226, 23.134022, 0.8954264, 3.1850672, 0.10089636]\n",
      "global_step 4101 , loss 1.1714684\n",
      "global_step 4201 , loss 1.1965053\n",
      "global_step 4301 , loss 1.2567177\n",
      "global_step 4401 , loss 1.7464006\n",
      "global_step 4501 , loss 1.3633778\n",
      "global_step 4601 , loss 1.2345294\n",
      "global_step 4701 , loss 0.8009938\n",
      "global_step 4801 , loss 1.098444\n",
      "global_step 4901 , loss 1.0592144\n",
      "global_step 5001 , loss 1.6236281\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  60.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.  94.]]\n",
      " true labels train [ 1.  1. 10.  1.  4.  1. 13.  1.  1. 28.]\n",
      " pred labels train [ 0.96383375  1.0914137  10.349976    0.92318606  1.0663099   1.1922535\n",
      " 15.913693    1.0321485   1.0049838  29.348621  ]\n",
      " all vars norms [77.2112, 0.6538005, 26.45561, 1.0417384, 3.638081, 0.12277805]\n",
      "global_step 5101 , loss 1.1934295\n",
      "global_step 5201 , loss 1.1525092\n",
      "global_step 5301 , loss 1.3034518\n",
      "global_step 5401 , loss 1.1913524\n",
      "global_step 5501 , loss 1.09308\n",
      "global_step 5601 , loss 0.88296115\n",
      "global_step 5701 , loss 0.8214344\n",
      "global_step 5801 , loss 0.780872\n",
      "global_step 5901 , loss 1.0887737\n",
      "global_step 6001 , loss 1.9262013\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  12.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   1.  68.]]\n",
      " true labels train [ 1. 72.  1. 53.  8. 48.  1. 49.  1.  1.]\n",
      " pred labels train [ 1.6377152  72.29225     0.76131344 49.193268    7.33863    42.98379\n",
      "  0.85780084 41.289238    0.53610814  0.8641422 ]\n",
      " all vars norms [81.4085, 0.66517663, 29.41364, 1.1233745, 3.9996614, 0.1432549]\n",
      "global_step 6101 , loss 0.7780068\n",
      "global_step 6201 , loss 1.2861075\n",
      "global_step 6301 , loss 1.2471733\n",
      "global_step 6401 , loss 0.93618894\n",
      "global_step 6501 , loss 0.9928986\n",
      "global_step 6601 , loss 0.9646708\n",
      "global_step 6701 , loss 0.9513073\n",
      "global_step 6801 , loss 0.8595091\n",
      "global_step 6901 , loss 0.7419089\n",
      "global_step 7001 , loss 0.7528395\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.  15.]]\n",
      " true labels train [  1.  10. 100.   1.   1.   1. 100.  10.  93.  10.]\n",
      " pred labels train [ 0.5355059  9.717671  99.07649    0.223955   1.0838202  1.3522352\n",
      " 98.41577    9.911494  94.60388   10.532477 ]\n",
      " all vars norms [85.520706, 0.66884387, 32.11329, 1.2185813, 4.29265, 0.16523631]\n",
      "global_step 7101 , loss 0.86737406\n",
      "global_step 7201 , loss 0.8139542\n",
      "global_step 7301 , loss 1.4180049\n",
      "global_step 7401 , loss 0.8520433\n",
      "global_step 7501 , loss 0.99173707\n",
      "global_step 7601 , loss 0.8757546\n",
      "global_step 7701 , loss 1.0907538\n",
      "global_step 7801 , loss 0.763569\n",
      "global_step 7901 , loss 0.927816\n",
      "global_step 8001 , loss 0.7552098\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   0.   1.   0.   0.   0.   0.  47.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  87.]]\n",
      " true labels train [13. 10. 81.  5. 83.  1. 14.  1. 87. 16.]\n",
      " pred labels train [11.986014  10.239142  83.96339    9.668067  84.74489    0.9886281\n",
      " 14.142867   1.2689611 89.8844    15.76129  ]\n",
      " all vars norms [89.45612, 0.6780746, 34.478065, 1.3253518, 4.515294, 0.18732722]\n",
      "global_step 8101 , loss 0.83044505\n",
      "global_step 8201 , loss 0.69128793\n",
      "global_step 8301 , loss 0.80536693\n",
      "global_step 8401 , loss 0.9685198\n",
      "global_step 8501 , loss 0.7053844\n",
      "global_step 8601 , loss 0.94343996\n",
      "global_step 8701 , loss 0.8160691\n",
      "global_step 8801 , loss 0.6168959\n",
      "global_step 8901 , loss 0.70647967\n",
      "global_step 9001 , loss 0.6767628\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  22.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   2.]]\n",
      " true labels train [  1. 100.   1. 100.   1.  14.  47.  10.  10.   1.]\n",
      " pred labels train [ 1.2284291  99.22898     0.89815176 97.661514    0.97211754 15.716976\n",
      " 48.29162     9.475869   10.081097    0.9339708 ]\n",
      " all vars norms [93.23164, 0.6808996, 36.686108, 1.403738, 4.6287017, 0.21463424]\n",
      "global_step 9101 , loss 1.0560172\n",
      "global_step 9201 , loss 0.977947\n",
      "global_step 9301 , loss 0.67808014\n",
      "global_step 9401 , loss 0.74289215\n",
      "global_step 9501 , loss 0.7815556\n",
      "global_step 9601 , loss 0.6381151\n",
      "global_step 9701 , loss 0.66504383\n",
      "global_step 9801 , loss 0.8205805\n",
      "global_step 9901 , loss 0.8093892\n",
      "global_step 10001 , loss 0.9138808\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   1.   0.   0.  69.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   5.]]\n",
      " true labels train [  1. 100.  10. 100.  10.  10.  10.   8.  68. 100.]\n",
      " pred labels train [ 1.0299884 99.1842     9.8237915 98.08215    9.996736   7.2217546\n",
      "  9.988881   7.733655  65.79937   98.70542  ]\n",
      " all vars norms [96.88071, 0.6882091, 38.92521, 1.4850162, 4.676082, 0.253282]\n",
      "global_step 10101 , loss 1.0546046\n",
      "global_step 10201 , loss 1.2938004\n",
      "global_step 10301 , loss 0.7150863\n",
      "global_step 10401 , loss 0.90731096\n",
      "global_step 10501 , loss 0.7941229\n",
      "global_step 10601 , loss 0.57562983\n",
      "global_step 10701 , loss 0.7239051\n",
      "global_step 10801 , loss 0.93602586\n",
      "global_step 10901 , loss 0.6409298\n",
      "global_step 11001 , loss 1.0560944\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  54.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.  14.]]\n",
      " true labels train [  1.  10.   1. 100.  10.  10.  10.   1.  10.  85.]\n",
      " pred labels train [ 1.060022    9.037763    1.055664   94.87349     9.74079    10.139022\n",
      "  9.290113    0.88908315  9.878618   81.56338   ]\n",
      " all vars norms [100.589806, 0.6838309, 41.21271, 1.5671711, 4.7063537, 0.30452812]\n",
      "global_step 11101 , loss 1.0020245\n",
      "global_step 11201 , loss 0.6961942\n",
      "global_step 11301 , loss 0.5847331\n",
      "global_step 11401 , loss 0.86345434\n",
      "global_step 11501 , loss 0.6474047\n",
      "global_step 11601 , loss 1.0833182\n",
      "global_step 11701 , loss 0.62850356\n",
      "global_step 11801 , loss 0.6881223\n",
      "global_step 11901 , loss 0.6956188\n",
      "global_step 12001 , loss 0.8666954\n",
      " true feats train [[100.  10.  10.  10. 100.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  21.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   1.   0.   0.   0.  96.]]\n",
      " true labels train [  1.   1.   1. 100.  10.   5.   1.   4.  85.   1.]\n",
      " pred labels train [  1.0122057   0.9815886   1.0182669 100.5365      9.744607    3.5932271\n",
      "   1.0983962   3.4608867  85.690994    1.7064292]\n",
      " all vars norms [104.092316, 0.6952635, 43.54221, 1.6403052, 4.7410145, 0.36160988]\n",
      "global_step 12101 , loss 0.79029584\n",
      "global_step 12201 , loss 0.63287127\n",
      "global_step 12301 , loss 0.57557327\n",
      "global_step 12401 , loss 0.805449\n",
      "global_step 12501 , loss 0.8898157\n",
      "global_step 12601 , loss 0.89389324\n",
      "global_step 12701 , loss 0.78903216\n",
      "global_step 12801 , loss 0.6848549\n",
      "global_step 12901 , loss 0.56376076\n",
      "global_step 13001 , loss 0.65377414\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  44.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   0.   1.   0.   0.   0.   0.  93.]]\n",
      " true labels train [ 10.  10.  10. 100.  13.   8. 100.   1.  10.   1.]\n",
      " pred labels train [  9.575908    9.564478   10.283623  101.82406    11.963297    7.700782\n",
      "  97.341576    1.0183127   9.917098    0.9826676]\n",
      " all vars norms [107.57456, 0.6977831, 46.100586, 1.7261739, 4.776221, 0.42832547]\n",
      "global_step 13101 , loss 0.664185\n",
      "global_step 13201 , loss 0.76998496\n",
      "global_step 13301 , loss 0.8805567\n",
      "global_step 13401 , loss 0.7907285\n",
      "global_step 13501 , loss 0.6150414\n",
      "global_step 13601 , loss 0.66224813\n",
      "global_step 13701 , loss 0.6072421\n",
      "global_step 13801 , loss 0.5985314\n",
      "global_step 13901 , loss 0.7072793\n",
      "global_step 14001 , loss 0.6458372\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   1.  69.]\n",
      " [100.  10.  10.  10. 100.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  95.]]\n",
      " true labels train [ 10. 100.   1.  10.   1.  33.   4.  81.   1.  52.]\n",
      " pred labels train [ 10.12327    102.74444      1.0236022    9.601294     0.89012057\n",
      "  32.35427      4.237751    77.33548      1.8578618   52.02911   ]\n",
      " all vars norms [110.93738, 0.7129489, 49.22194, 1.7945668, 4.8311844, 0.501424]\n",
      "global_step 14101 , loss 0.7078202\n",
      "global_step 14201 , loss 0.8625694\n",
      "global_step 14301 , loss 0.7274066\n",
      "global_step 14401 , loss 0.62691295\n",
      "global_step 14501 , loss 0.5878896\n",
      "global_step 14601 , loss 0.60147935\n",
      "global_step 14701 , loss 0.7630906\n",
      "global_step 14801 , loss 0.54869556\n",
      "global_step 14901 , loss 0.67795587\n",
      "global_step 15001 , loss 0.6004607\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.  44.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.  96.]]\n",
      " true labels train [  1.   1.  10.   1. 100.  10.  10.   1.  48.   8.]\n",
      " pred labels train [ 1.0206299  1.0232649 10.056986   0.9977298 99.360115  10.76857\n",
      " 10.094864   1.2161694 51.760143   8.325221 ]\n",
      " all vars norms [114.10999, 0.72242224, 52.178165, 1.8650851, 4.928749, 0.5704349]\n",
      "global_step 15101 , loss 0.8291634\n",
      "global_step 15201 , loss 0.55537635\n",
      "global_step 15301 , loss 0.6593728\n",
      "global_step 15401 , loss 0.52564526\n",
      "global_step 15501 , loss 0.5639094\n",
      "global_step 15601 , loss 0.7385466\n",
      "global_step 15701 , loss 0.50668365\n",
      "global_step 15801 , loss 0.5572022\n",
      "global_step 15901 , loss 0.9179211\n",
      "global_step 16001 , loss 0.62310374\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   1.   0.   0.   0.   0.   0.   0.   1.  34.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   1.   0.   0.   0.   1.   0.   0.   0.  92.]]\n",
      " true labels train [32. 13.  1. 14. 38. 10.  1.  1. 49. 10.]\n",
      " pred labels train [33.23251   13.338326   0.988251  14.005684  38.86465    9.809786\n",
      "  1.1426407  0.9977997 50.4701     9.649697 ]\n",
      " all vars norms [117.34887, 0.7258608, 54.970592, 1.9644772, 4.9765663, 0.64459854]\n",
      "global_step 16101 , loss 0.67419213\n",
      "global_step 16201 , loss 0.7033189\n",
      "global_step 16301 , loss 0.69815123\n",
      "global_step 16401 , loss 0.651688\n",
      "global_step 16501 , loss 0.46896842\n",
      "global_step 16601 , loss 0.84170914\n",
      "global_step 16701 , loss 0.59159994\n",
      "global_step 16801 , loss 0.5599954\n",
      "global_step 16901 , loss 0.7349579\n",
      "global_step 17001 , loss 0.5937425\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   2.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   1.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.  57.]]\n",
      " true labels train [  1.  59. 100. 100.  35.   1.   1. 100.   1.  17.]\n",
      " pred labels train [ 1.0084772 58.939117  99.621826  99.28806   37.999466   1.0112525\n",
      "  0.8671314 99.17418    0.9850434 16.155087 ]\n",
      " all vars norms [120.52689, 0.73310333, 57.702915, 2.035773, 4.98432, 0.7145797]\n",
      "global_step 17101 , loss 0.5152513\n",
      "global_step 17201 , loss 0.4804311\n",
      "global_step 17301 , loss 0.5075445\n",
      "global_step 17401 , loss 0.55207676\n",
      "global_step 17501 , loss 0.63739866\n",
      "global_step 17601 , loss 0.5843184\n",
      "global_step 17701 , loss 0.59989345\n",
      "global_step 17801 , loss 0.55862033\n",
      "global_step 17901 , loss 0.4911424\n",
      "global_step 18001 , loss 0.6063921\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   1.   0.   0.   0.  43.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  33.]]\n",
      " true labels train [ 93.  10.   1.   1.  10. 100.  47. 100.  48.   1.]\n",
      " pred labels train [ 92.8311      9.738369    0.9909112   0.6055898  10.04848   100.43309\n",
      "  46.935047   99.93895    48.374916    0.838648 ]\n",
      " all vars norms [123.71574, 0.73662055, 60.246807, 2.1066923, 4.984423, 0.7908653]\n",
      "global_step 18101 , loss 0.5639852\n",
      "global_step 18201 , loss 0.46553186\n",
      "global_step 18301 , loss 0.3779804\n",
      "global_step 18401 , loss 0.71991646\n",
      "global_step 18501 , loss 0.5575053\n",
      "global_step 18601 , loss 0.48144883\n",
      "global_step 18701 , loss 0.48769414\n",
      "global_step 18801 , loss 0.52578765\n",
      "global_step 18901 , loss 0.62953854\n",
      "global_step 19001 , loss 0.51600456\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  46.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   1.  38.]]\n",
      " true labels train [  1.  10.   1.  83.  18.  56.  10. 100.  10.  48.]\n",
      " pred labels train [ 1.0065111  9.875832   1.0126702 79.79524   20.69005   55.437836\n",
      " 10.181783  99.38699    9.958669  48.77231  ]\n",
      " all vars norms [126.451126, 0.7427818, 62.493736, 2.169398, 4.9714704, 0.8665681]\n",
      "global_step 19101 , loss 0.47557002\n",
      "global_step 19201 , loss 0.5174157\n",
      "global_step 19301 , loss 0.48377115\n",
      "global_step 19401 , loss 0.60738903\n",
      "global_step 19501 , loss 0.4654702\n",
      "global_step 19601 , loss 0.5931848\n",
      "global_step 19701 , loss 0.6621083\n",
      "global_step 19801 , loss 0.82117176\n",
      "global_step 19901 , loss 0.4623908\n",
      "global_step 20001 , loss 0.5035348\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.  83.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   0.   1.   0.   0.   0.   0.  17.]]\n",
      " true labels train [ 1. 11.  1. 49. 10. 30. 11.  1. 10.  6.]\n",
      " pred labels train [ 1.0099337 11.559148   1.0108727 48.390972   9.916163  30.481236\n",
      " 11.1913595  1.0190699  9.876904   6.1919904]\n",
      " all vars norms [129.26103, 0.7450229, 65.01838, 2.2228885, 4.9795694, 0.9569475]\n",
      "global_step 20101 , loss 0.4534911\n",
      "global_step 20201 , loss 0.5774858\n",
      "global_step 20301 , loss 0.58993244\n",
      "global_step 20401 , loss 0.4628412\n",
      "global_step 20501 , loss 0.6064515\n",
      "global_step 20601 , loss 0.68538713\n",
      "global_step 20701 , loss 0.48067862\n",
      "global_step 20801 , loss 0.7833799\n",
      "global_step 20901 , loss 0.79936475\n",
      "global_step 21001 , loss 0.7130442\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   8.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  65.]]\n",
      " true labels train [ 4.  1.  1. 33. 10. 65.  1.  1. 10.  1.]\n",
      " pred labels train [ 3.7857423   0.99934167  1.5807374  30.193964   10.832293   65.986115\n",
      "  0.99934167  1.1149946   9.740112    0.9989439 ]\n",
      " all vars norms [132.07527, 0.7535747, 67.24796, 2.289244, 4.9376473, 1.0002537]\n",
      "global_step 21101 , loss 0.46000952\n",
      "global_step 21201 , loss 0.55241305\n",
      "global_step 21301 , loss 0.5218408\n",
      "global_step 21401 , loss 0.5192252\n",
      "global_step 21501 , loss 0.5991657\n",
      "global_step 21601 , loss 0.5673305\n",
      "global_step 21701 , loss 0.52438337\n",
      "global_step 21801 , loss 0.6111862\n",
      "global_step 21901 , loss 0.532145\n",
      "global_step 22001 , loss 0.67615366\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   0.   0.   1.  97.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   1.   0.   0.  69.]]\n",
      " true labels train [100.   1.  10.   1.  30.  89.  10.   1.  72.  10.]\n",
      " pred labels train [102.03777    1.001577  10.052831   1.001577  27.049732  92.6436\n",
      "   9.867508   1.001577  72.49795    9.980513]\n",
      " all vars norms [134.97093, 0.7573864, 69.34605, 2.3577404, 4.891848, 1.0009731]\n",
      "global_step 22101 , loss 0.69716513\n",
      "global_step 22201 , loss 0.45061558\n",
      "global_step 22301 , loss 0.40886956\n",
      "global_step 22401 , loss 0.45692575\n",
      "global_step 22501 , loss 0.5471767\n",
      "global_step 22601 , loss 0.5874455\n",
      "global_step 22701 , loss 0.48075727\n",
      "global_step 22801 , loss 0.52934575\n",
      "global_step 22901 , loss 0.6044755\n",
      "global_step 23001 , loss 0.69715506\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  65.]\n",
      " [100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   0.  26.]]\n",
      " true labels train [ 10.   1. 100. 100.   1.  89.   1.   1. 100.   1.]\n",
      " pred labels train [ 10.42648     1.2712864  98.978645  102.77179     0.9999627  91.2257\n",
      "   0.9999627   1.1656815 102.777145    0.9999627]\n",
      " all vars norms [138.0781, 0.7614688, 71.414665, 2.4389944, 4.8301444, 1.0004089]\n",
      "global_step 23101 , loss 0.51225686\n",
      "global_step 23201 , loss 0.53485274\n",
      "global_step 23301 , loss 0.5182524\n",
      "global_step 23401 , loss 0.665325\n",
      "global_step 23501 , loss 0.5468907\n",
      "global_step 23601 , loss 0.47297814\n",
      "global_step 23701 , loss 0.58368933\n",
      "global_step 23801 , loss 0.5973894\n",
      "global_step 23901 , loss 0.65355146\n",
      "global_step 24001 , loss 0.7656983\n",
      " true feats train [[100.  10.  10.  10. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   1.   0.   0.   0.  44.]\n",
      " [100.  10.  10.  10. 100.   0.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   1.  56.]]\n",
      " true labels train [ 93.  56.   1.  10.   4.  53.   2.   1. 100.   1.]\n",
      " pred labels train [90.14835   54.643528   0.943582   9.890346   4.1199875 52.032127\n",
      "  2.5304365  0.9997447 98.62222    0.9997447]\n",
      " all vars norms [140.74222, 0.766558, 73.62965, 2.5009077, 4.7732806, 1.0011227]\n",
      "global_step 24101 , loss 0.38697678\n",
      "global_step 24201 , loss 0.43807143\n",
      "global_step 24301 , loss 0.6722052\n",
      "global_step 24401 , loss 0.51685965\n",
      "global_step 24501 , loss 0.5243528\n"
     ]
    }
   ],
   "source": [
    "# Train.\n",
    "sess_args = {\n",
    "#              'checkpoint_dir': os.getcwd(),\n",
    "#              'save_checkpoint_secs': 60, \n",
    "             'save_summaries_secs': 1,\n",
    "             'log_step_count_steps': 5000,\n",
    "             'hooks': [tf.train.NanTensorHook(loss),\n",
    "                       tf.train.StopAtStepHook(last_step=2000000),\n",
    "#                       tf.train.LoggingTensorHook([global_step, loss], every_n_iter=500)\n",
    "                      ],\n",
    "            }\n",
    "\n",
    "loss_vals = collections.deque(maxlen=40)\n",
    "i = 0\n",
    "with tf.train.MonitoredTrainingSession(**sess_args) as sess:\n",
    "    sess.run(iterator_init)\n",
    "    while not sess.should_stop():\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     running_test_loss = 0.0\n",
    "#     while True:\n",
    "        _, loss_val, global_step_val, true_feat_train, true_labels_train, pred_labels_train = sess.run(\n",
    "            [train_step, loss, global_step,  feature_batch, label_batch, out])\n",
    "\n",
    "        if i % 100 == 0:\n",
    "#             sess.run(test_iterator_init)\n",
    "#             test_loss_val, actual_labels, predicted_labels = sess.run([loss, label_batch, out])\n",
    "\n",
    "#             loss_vals.append(test_loss_val)\n",
    "#             avg_test_loss = np.mean(loss_vals)\n",
    "#             print('global_step', global_step_val, ', loss', loss_val, ', avg test loss', avg_test_loss)\n",
    "            print('global_step', global_step_val, ', loss', loss_val)\n",
    "            if i % 1000 == 0:\n",
    "#                 print(' true labels', actual_labels[:20].reshape(-1,))\n",
    "#                 print(' pred labels', predicted_labels[:20].reshape(-1,))\n",
    "                print(' true feats train', true_feat_train[:2])\n",
    "                print(' true labels train', true_labels_train[:10].reshape(-1,))\n",
    "                print(' pred labels train', pred_labels_train[:10].reshape(-1,))\n",
    "                print(' all vars norms', sess.run(all_vars_norms))\n",
    "        i += 1\n",
    "#         if i >= 10000:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
