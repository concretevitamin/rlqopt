{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zongheng/anaconda/envs/ray-0321/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "# TODO(zongheng): visualization via tensorboard.\n",
    "# TODO(zongheng): print some prediction from test set\n",
    "\n",
    "# With gaussian weights.\n",
    "# 500 training pts; 1000 testing.\n",
    "PATH = \"5rel-10numAttrs-22totalAttrs-100maxTblSize-10buckets-3000.csv\"\n",
    "NUM_FIELDS = 34\n",
    "TRAIN_PATH = \"data/train-{}\".format(PATH)\n",
    "TEST_PATH = \"data/test-{}\".format(PATH)\n",
    "net = [64,32]; lr = 1e-2\n",
    "net = [64, 16, 64]; lr = 5e-3\n",
    "net = [64,64]; lr =  1e-3\n",
    "batch_size = 500\n",
    "num_steps = 5500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.float32)\n",
      "(TensorShape([Dimension(None), Dimension(33)]), TensorShape([Dimension(None)]))\n",
      "(tf.float32, tf.float32)\n",
      "(TensorShape([Dimension(None), Dimension(33)]), TensorShape([Dimension(None)]))\n"
     ]
    }
   ],
   "source": [
    "def DatasetFromPath(pathname):\n",
    "    dataset = tf.data.TextLineDataset(pathname)\n",
    "\n",
    "    def ParseLine(line):\n",
    "        fields = tf.decode_csv(line, [[0.0]] * NUM_FIELDS)\n",
    "        # Last number is label.\n",
    "        return tf.reshape(tf.stack(fields[:-1]), [-1]), fields[-1]\n",
    "\n",
    "    def MeanCenterBatch(feat, lab):\n",
    "        feat_mean, feat_var = tf.nn.moments(feat, axes=[0])\n",
    "        lab_mean, lab_var = tf.nn.moments(lab, axes=[0])\n",
    "        feat = (feat - feat_mean) / (tf.sqrt(feat_var) + 1e-8)\n",
    "        lab = (lab - lab_mean) / (tf.sqrt(lab_var) + 1e-8)\n",
    "        \n",
    "#         feat = (feat - feat_mean) / (feat_var + 1e-3)\n",
    "#         lab = (lab - lab_mean) / (lab_var + 1e-3)\n",
    "#         feat = tf.Print(feat, [feat_mean, feat_var], message='feat mean/var: ')\n",
    "#         lab = tf.Print(lab, [lab_mean, lab_var], message='lab mean/var: ')\n",
    "        return feat, lab\n",
    "    \n",
    "#     # K-point dataset, for sanity checking.\n",
    "#     K = 2\n",
    "#     dataset = dataset.take(K).map(ParseLine)\n",
    "#     dataset = dataset.shuffle(20000, reshuffle_each_iteration=False) \\\n",
    "#         .batch(K) \\\n",
    "#         .repeat()\n",
    "\n",
    "    dataset = dataset.cache().map(ParseLine)\n",
    "    dataset = dataset.shuffle(20000, reshuffle_each_iteration=True) \\\n",
    "        .repeat() \\\n",
    "        .batch(batch_size) \\\n",
    "        .map(MeanCenterBatch) \n",
    "\n",
    "    print(dataset.output_types)\n",
    "    print(dataset.output_shapes)\n",
    "    return dataset\n",
    "\n",
    "# Train.\n",
    "dataset = DatasetFromPath(TRAIN_PATH)\n",
    "iterator = tf.data.Iterator.from_structure(\n",
    "    dataset.output_types, dataset.output_shapes)\n",
    "iterator_init = iterator.make_initializer(dataset)\n",
    "# Test.\n",
    "test_dataset = DatasetFromPath(TEST_PATH)\n",
    "test_iterator_init = iterator.make_initializer(test_dataset)\n",
    "# Shared.\n",
    "feature_batch, label_batch = iterator.get_next()\n",
    "\n",
    "    \n",
    "\n",
    "# mean, var = tf.nn.moments(feature_batch, axes=[0])\n",
    "# feature_batch = (feature_batch - mean) / tf.sqrt(var)\n",
    "# mean, var = tf.nn.moments(label_batch, axes=[0])\n",
    "# label_batch = (label_batch - mean) / var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Train Loss is illegal; using Train_Loss instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0 is illegal; using dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0 is illegal; using dense/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0 is illegal; using dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0 is illegal; using dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0 is illegal; using dense_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0 is illegal; using dense_2/bias_0 instead.\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Graph definition.\n",
    "dense = feature_batch\n",
    "for i in range(len(net)):\n",
    "    dense = tf.layers.dense(\n",
    "        dense,\n",
    "        net[i],\n",
    "#         kernel_initializer=tf.truncated_normal_initializer(stddev=20.0),\n",
    "#         kernel_initializer=tf.initializers.variance_scaling(scale=10.0),\n",
    "        kernel_initializer=tf.initializers.variance_scaling(),\n",
    "#         kernel_initializer=tf.initializers.orthogonal(),\n",
    "#         kernel_initializer=tf.initializers.uniform_unit_scaling(),\n",
    "#         kernel_initializer=tf.initializers.random_normal(stddev=10),\n",
    "#         kernel_initializer=tf.initializers.random_uniform(-100, 100),\n",
    "#         kernel_initializer=tf.initializers.zeros(),\n",
    "#         bias_initializer=tf.initializers.zeros(),\n",
    "#         activation=tf.nn.sigmoid,\n",
    "        activation=tf.nn.relu,\n",
    "#         activation=tf.nn.tanh,\n",
    "    )\n",
    "out = tf.layers.dense(\n",
    "    dense,\n",
    "    1,\n",
    "#         kernel_initializer=tf.truncated_normal_initializer(stddev=20.0),\n",
    "#         kernel_initializer=tf.initializers.variance_scaling(scale=10.0),\n",
    "#         kernel_initializer=tf.initializers.orthogonal(),\n",
    "#         kernel_initializer=tf.initializers.uniform_unit_scaling(),\n",
    "#         kernel_initializer=tf.initializers.random_normal(stddev=10),\n",
    "        kernel_initializer=tf.initializers.variance_scaling(),\n",
    "#         kernel_initializer=tf.initializers.random_uniform(-100, 100),\n",
    "#         kernel_initializer=tf.initializers.zeros(),\n",
    "#         bias_initializer=tf.initializers.zeros(),\n",
    ")\n",
    "\n",
    "\n",
    "all_vars = tf.trainable_variables()\n",
    "# l1_regularizer = tf.contrib.layers.l1_regularizer(scale=0.05)\n",
    "# regularization_penalty = tf.contrib.layers.apply_regularization(l1_regularizer, all_vars)\n",
    "\n",
    "# NOTE: critical to reshape / or keep the order correct...\n",
    "# out: [B, 1]\n",
    "# label_batch [B]\n",
    "# loss = tf.reduce_mean(tf.square(out - label_batch))\n",
    "# loss = tf.reduce_mean(tf.square(out - label_batch)) \n",
    "loss = tf.reduce_mean(tf.square(tf.reshape(out,[-1]) - label_batch)) \n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "train_step = optimizer.minimize(\n",
    "    loss, global_step=global_step)\n",
    "\n",
    "# For visualization.\n",
    "tf.summary.scalar('Train Loss', loss)\n",
    "for v in all_vars:\n",
    "    tf.summary.histogram(v.name, v)\n",
    "all_vars_norms = [tf.norm(v) for v in all_vars]\n",
    "print(len(all_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "global_step 1 , loss 1.0930773\n",
      " true feats train [[ 0.          0.          0.          0.          0.         -0.23891887\n",
      "  -0.18760797 -0.18181819 -0.2245442  -0.1932471  -0.1758629  -0.2145347\n",
      "  -0.175863   -0.17586307 -0.18181801 -0.15681244 -0.18181789  3.889616\n",
      "  -0.27854282 -0.27854276 -0.19874893 -0.2435616  -0.27435106 -0.23420511\n",
      "  -0.20412378 -0.23891889 -0.2481351   0.          0.          0.\n",
      "   0.          0.         -1.3734459 ]\n",
      " [ 0.          0.          0.          0.          0.         -0.23891887\n",
      "  -0.18760797 -0.18181819 -0.2245442  -0.1932471  -0.1758629  -0.2145347\n",
      "  -0.175863   -0.17586307 -0.18181801 -0.15681244 -0.18181789 -0.25709614\n",
      "  -0.27854282 -0.27854276 -0.19874893 -0.2435616  -0.27435106 -0.23420511\n",
      "   4.8989706  -0.23891889 -0.2481351   0.          0.          0.\n",
      "   0.          0.         -0.6885168 ]]\n",
      " true labels train [-0.51539123  0.6626459  -0.51539123  0.9571551  -0.51539123  0.36813658\n",
      " -0.51539123 -0.51539123  0.46630633  0.36813658]\n",
      " pred labels train [0.00550798 0.07058522 0.10373175 0.4205854  0.10626432 0.49615163\n",
      " 0.11158021 0.22957188 0.40706247 0.11845598]\n",
      " all vars norms [7.0584264, 0.00799876, 7.0104356, 0.00799954, 0.9183297, 0.0009999992]\n",
      "global_step 101 , loss 0.6159095\n",
      "global_step 201 , loss 0.4052665\n",
      "global_step 301 , loss 0.24166127\n",
      "global_step 401 , loss 0.14204039\n",
      "global_step 501 , loss 0.098347336\n",
      "global_step 601 , loss 0.07670509\n",
      "global_step 701 , loss 0.06561255\n",
      "global_step 801 , loss 0.057017792\n",
      "global_step 901 , loss 0.050767712\n",
      "global_step 1001 , loss 0.04613861\n",
      " true feats train [[ 0.          0.          0.          0.          0.         -0.23891883\n",
      "  -0.1876081  -0.1818179  -0.22454418  5.174724   -0.17586304 -0.2145347\n",
      "  -0.17586309 -0.175863   -0.18181807 -0.15681244 -0.18181808 -0.25709587\n",
      "  -0.27854273 -0.27854276 -0.1987489  -0.24356166 -0.27435136 -0.2342053\n",
      "  -0.20412384 -0.23891875 -0.24813491  0.          0.          0.\n",
      "   0.          0.          1.5619648 ]\n",
      " [ 0.          0.          0.          0.          0.         -0.23891883\n",
      "  -0.1876081  -0.1818179  -0.22454418 -0.19324695 -0.17586304 -0.2145347\n",
      "  -0.17586309 -0.175863   -0.18181807 -0.15681244 -0.18181808 -0.25709587\n",
      "   3.5901065  -0.27854276 -0.1987489  -0.24356166 -0.27435136 -0.2342053\n",
      "  -0.20412384 -0.23891875 -0.24813491  0.          0.          0.\n",
      "   0.          0.         -1.2103678 ]]\n",
      " true labels train [ 0.36813658  0.46630633 -0.51539123  0.5644761  -0.51539123 -0.3190517\n",
      " -0.51539123  0.17179707 -0.3190517  -0.51539123]\n",
      " pred labels train [ 0.35236245  0.4449197  -0.5362789   0.57408756 -0.5344488  -0.3405431\n",
      " -0.5418063   0.10797362 -0.38382453 -0.53983325]\n",
      " all vars norms [9.717904, 0.5899349, 12.329692, 0.28391153, 3.221244, 0.024865584]\n",
      "global_step 1101 , loss 0.040954858\n",
      "global_step 1201 , loss 0.037099414\n",
      "global_step 1301 , loss 0.03379902\n",
      "global_step 1401 , loss 0.03316672\n",
      "global_step 1501 , loss 0.02815977\n",
      "global_step 1601 , loss 0.02561273\n",
      "global_step 1701 , loss 0.023471734\n",
      "global_step 1801 , loss 0.021524778\n",
      "global_step 1901 , loss 0.019072676\n",
      "global_step 2001 , loss 0.017047198\n",
      " true feats train [[ 0.          0.          0.          0.          0.         -0.2389188\n",
      "  -0.18760808 -0.18181804 -0.22454418  5.1747313  -0.17586271 -0.2145347\n",
      "  -0.17586312 -0.17586304 -0.18181793 -0.15681247 -0.18181784 -0.2570958\n",
      "  -0.27854276 -0.27854276 -0.19874902 -0.24356155 -0.27435154 -0.23420529\n",
      "  -0.20412385 -0.23891869 -0.24813467  0.          0.          0.\n",
      "   0.          0.         -0.85159516]\n",
      " [ 0.          0.          0.          0.          0.         -0.2389188\n",
      "  -0.18760808 -0.18181804 -0.22454418 -0.19324723 -0.17586271 -0.2145347\n",
      "  -0.17586312 -0.17586304 -0.18181793 -0.15681247 -0.18181784 -0.2570958\n",
      "   3.590107   -0.27854276 -0.19874902 -0.24356155 -0.27435154 -0.23420529\n",
      "  -0.20412385 -0.23891869 -0.24813467  0.          0.          0.\n",
      "   0.          0.         -0.55805415]]\n",
      " true labels train [ 0.26996684 -0.51539123 -0.51539123  0.26996684  0.85898536 -0.51539123\n",
      " -0.51539123 -0.51539123  0.5644761  -0.51539123]\n",
      " pred labels train [ 0.27334967 -0.52416843 -0.5154707   0.25429884  0.8482196  -0.5200055\n",
      " -0.51763207 -0.5290769   0.5878379  -0.526089  ]\n",
      " all vars norms [10.859536, 0.70248175, 13.736635, 0.3027249, 3.5536377, 0.024862664]\n",
      "global_step 2101 , loss 0.015849087\n",
      "global_step 2201 , loss 0.014593118\n",
      "global_step 2301 , loss 0.012141665\n",
      "global_step 2401 , loss 0.010708865\n",
      "global_step 2501 , loss 0.009464421\n",
      "global_step 2601 , loss 0.0084147705\n",
      "global_step 2701 , loss 0.0073809717\n",
      "global_step 2801 , loss 0.007893683\n",
      "global_step 2901 , loss 0.005880484\n",
      "global_step 3001 , loss 0.0052001644\n",
      " true feats train [[ 0.          0.          0.          0.          0.         -0.23891883\n",
      "  -0.1876081  -0.18181814 -0.2245442  -0.19324724 -0.17586286 -0.21453469\n",
      "  -0.1758628  -0.17586283 -0.18181792 -0.15681244 -0.18181789 -0.25709593\n",
      "  -0.27854276 -0.27854276 -0.19874895 -0.2435615   3.6449516  -0.23420542\n",
      "  -0.20412382 -0.2389189  -0.24813488  0.          0.          0.\n",
      "   0.          0.         -1.1125207 ]\n",
      " [ 0.          0.          0.          0.          0.         -0.23891883\n",
      "  -0.1876081  -0.18181814 -0.2245442  -0.19324724 -0.17586286 -0.21453469\n",
      "  -0.1758628  -0.17586283 -0.18181792 -0.15681244 -0.18181789 -0.25709593\n",
      "  -0.27854276 -0.27854276  5.0314865  -0.2435615  -0.27435118 -0.23420542\n",
      "  -0.20412382 -0.2389189  -0.24813488  0.          0.          0.\n",
      "   0.          0.          0.5834944 ]]\n",
      " true labels train [-0.22088195 -0.51539123 -0.51539123  0.5644761  -0.51539123 -0.51539123\n",
      " -0.22088195  0.5644761  -0.51539123  0.9571551 ]\n",
      " pred labels train [-0.2799217  -0.5250059  -0.5262746   0.56821865 -0.51915175 -0.53789395\n",
      " -0.21453065  0.53711444 -0.53789395  0.9475573 ]\n",
      " all vars norms [11.597483, 0.7691611, 14.804048, 0.31437764, 3.7182617, 0.026547171]\n",
      "global_step 3101 , loss 0.0043515526\n",
      "global_step 3201 , loss 0.003738494\n",
      "global_step 3301 , loss 0.0036309592\n",
      "global_step 3401 , loss 0.0028451835\n",
      "global_step 3501 , loss 0.0025654293\n",
      "global_step 3601 , loss 0.005675535\n",
      "global_step 3701 , loss 0.003302389\n",
      "global_step 3801 , loss 0.0019181081\n",
      "global_step 3901 , loss 0.002954431\n",
      "global_step 4001 , loss 0.0014223907\n",
      " true feats train [[ 0.          0.          0.          0.          0.         -0.23891881\n",
      "  -0.187608   -0.18181792 -0.2245442  -0.19324695 -0.1758629  -0.21453469\n",
      "  -0.1758629  -0.17586297 -0.18181789 -0.15681244 -0.18181801 -0.2570958\n",
      "  -0.27854273 -0.27854273 -0.19874895  4.105751   -0.2743514  -0.23420548\n",
      "  -0.2041239  -0.2389189  -0.24813479  0.          0.          0.\n",
      "   0.          0.          0.5834943 ]\n",
      " [ 0.          0.          0.          0.          0.         -0.23891881\n",
      "  -0.187608   -0.18181792 -0.2245442  -0.19324695 -0.1758629  -0.21453469\n",
      "  -0.1758629  -0.17586297 -0.18181789 -0.15681244 -0.18181801 -0.2570958\n",
      "  -0.27854273  3.5901065  -0.19874895 -0.2435615  -0.2743514  -0.23420548\n",
      "  -0.2041239  -0.2389189  -0.24813479  0.          0.          0.\n",
      "   0.          0.          1.6924272 ]]\n",
      " true labels train [-0.51539123 -0.51539123 -0.41722146  0.46630633  8.319887   -0.51539123\n",
      " -0.51539123 -0.51539123 -0.51539123  0.6626459 ]\n",
      " pred labels train [-0.5158797  -0.53124434 -0.42833883  0.45354098  8.29767    -0.48163158\n",
      " -0.52699167 -0.51509434 -0.53124434  0.65335363]\n",
      " all vars norms [12.009073, 0.7990235, 15.534475, 0.3183518, 3.797913, 0.0281739]\n",
      "global_step 4101 , loss 0.006363229\n",
      "global_step 4201 , loss 0.0010067364\n",
      "global_step 4301 , loss 0.0013021339\n",
      "global_step 4401 , loss 0.002365028\n",
      "global_step 4501 , loss 0.0009464159\n",
      "global_step 4601 , loss 0.0008181501\n",
      "global_step 4701 , loss 0.0030025668\n",
      "global_step 4801 , loss 0.00061372336\n",
      "global_step 4901 , loss 0.00079113414\n",
      "global_step 5001 , loss 0.0005295705\n",
      " true feats train [[ 0.          0.          0.          0.          0.         -0.2389189\n",
      "  -0.18760803 -0.1818179  -0.2245442  -0.19324717 -0.17586295 -0.2145347\n",
      "  -0.17586295 -0.17586328 -0.18181798 -0.15681244 -0.18181796 -0.2570961\n",
      "  -0.27854285 -0.27854276 -0.1987489  -0.24356157  3.6449575  -0.23420547\n",
      "  -0.20412378 -0.23891887 -0.24813496  0.          0.          0.\n",
      "   0.          0.         -1.1777521 ]\n",
      " [ 0.          0.          0.          0.          0.         -0.2389189\n",
      "  -0.18760803 -0.1818179  -0.2245442  -0.19324717 -0.17586295 -0.2145347\n",
      "  -0.17586295 -0.17586328 -0.18181798 -0.15681244 -0.18181796 -0.2570961\n",
      "  -0.27854285 -0.27854276 -0.1987489   4.105752   -0.27435163 -0.23420547\n",
      "  -0.20412378 -0.23891887 -0.24813496  0.          0.          0.\n",
      "   0.          0.          1.333655  ]]\n",
      " true labels train [-0.51539123 -0.51539123 -0.51539123  0.5644761  -0.51539123 -0.51539123\n",
      " -0.51539123  0.7608156   0.17179707 -0.51539123]\n",
      " pred labels train [-0.40282404 -0.51259196 -0.5115725   0.5671488  -0.5151645  -0.51960146\n",
      " -0.5122241   0.8103508   0.17515184 -0.5111333 ]\n",
      " all vars norms [12.186564, 0.8132022, 15.912397, 0.3201108, 3.8302271, 0.028838433]\n",
      "global_step 5101 , loss 0.0004931899\n",
      "global_step 5201 , loss 0.0004735805\n",
      "global_step 5301 , loss 0.0017254588\n",
      "global_step 5401 , loss 0.0018577712\n",
      "global_step 5501 , loss 0.0006599077\n",
      "global_step 5601 , loss 0.00039148578\n",
      "global_step 5701 , loss 0.00037523883\n",
      "global_step 5801 , loss 0.0003643982\n",
      "global_step 5901 , loss 0.00036023784\n",
      "global_step 6001 , loss 0.0032420172\n",
      " true feats train [[ 0.          0.          0.          0.          0.         -0.2389188\n",
      "  -0.18760803 -0.1818178  -0.22454418 -0.193247   -0.1758628  -0.2145347\n",
      "  -0.17586297 -0.17586304 -0.18181816 -0.15681244 -0.18181792 -0.25709608\n",
      "  -0.27854276 -0.27854276  5.0314884  -0.2435615  -0.27435157 -0.2342053\n",
      "  -0.20412394 -0.23891889 -0.24813487  0.          0.          0.\n",
      "   0.          0.          0.9422668 ]\n",
      " [ 0.          0.          0.          0.          0.         -0.2389188\n",
      "  -0.18760803 -0.1818178  -0.22454418 -0.193247   -0.1758628  -0.2145347\n",
      "  -0.17586297 -0.17586304 -0.18181816 -0.15681244 -0.18181792 -0.25709608\n",
      "  -0.27854276 -0.27854276 -0.198749    4.105751   -0.27435157 -0.2342053\n",
      "  -0.20412394 -0.23891889 -0.24813487  0.          0.          0.\n",
      "   0.          0.          1.0401138 ]]\n",
      " true labels train [-0.51539123 -0.51539123  0.6626459  -0.51539123 -0.51539123 -0.51539123\n",
      " -0.51539123  0.36813658  0.26996684 -0.51539123]\n",
      " pred labels train [-0.55713457 -0.57981867  0.6039794  -0.5674252  -0.54605824 -0.54513663\n",
      " -0.5818834   0.37025863  0.21220738 -0.5868787 ]\n",
      " all vars norms [12.261763, 0.8199774, 16.113626, 0.32044676, 3.8438988, 0.02937911]\n",
      "global_step 6101 , loss 0.00032596645\n",
      "global_step 6201 , loss 0.0003244049\n",
      "global_step 6301 , loss 0.0016976912\n",
      "global_step 6401 , loss 0.00031368455\n",
      "global_step 6501 , loss 0.00029567728\n",
      "global_step 6601 , loss 0.0002889769\n",
      "global_step 6701 , loss 0.00030591455\n",
      "global_step 6801 , loss 0.00072593644\n",
      "global_step 6901 , loss 0.00032034787\n",
      "global_step 7001 , loss 0.000285594\n",
      " true feats train [[ 0.          0.          0.          0.          0.         -0.23891881\n",
      "  -0.1876081  -0.18181813 -0.22454418  5.1747274  -0.17586301 -0.2145347\n",
      "  -0.17586301 -0.17586313 -0.18181776 -0.15681244 -0.18181802 -0.25709587\n",
      "  -0.27854276 -0.27854282 -0.1987489  -0.24356155 -0.27435136 -0.23420538\n",
      "  -0.20412377 -0.2389188  -0.24813506  0.          0.          0.\n",
      "   0.          0.         -0.10143476]\n",
      " [ 0.          0.          0.          0.          0.         -0.23891881\n",
      "  -0.1876081  -0.18181813 -0.22454418 -0.19324706 -0.17586301 -0.2145347\n",
      "  -0.17586301 -0.17586313 -0.18181776 -0.15681244 -0.18181802 -0.25709587\n",
      "  -0.27854276 -0.27854282 -0.1987489   4.1057515  -0.27435136 -0.23420538\n",
      "  -0.20412377 -0.2389188  -0.24813506  0.          0.          0.\n",
      "   0.          0.         -1.275599  ]]\n",
      " true labels train [ 0.9571551   0.46630633  0.5644761   0.26996684 -0.51539123  0.85898536\n",
      " -0.51539123 -0.3190517   0.36813658 -0.41722146]\n",
      " pred labels train [ 0.96002483  0.47254363  0.5677742   0.27536455 -0.51043737  0.8639661\n",
      " -0.5113104  -0.32558236  0.37316814 -0.44628796]\n",
      " all vars norms [12.290025, 0.8210338, 16.232283, 0.32092693, 3.8498657, 0.029664407]\n",
      "global_step 7101 , loss 0.000806523\n",
      "global_step 7201 , loss 0.00054976606\n",
      "global_step 7301 , loss 0.0002712557\n",
      "global_step 7401 , loss 0.00036360344\n",
      "global_step 7501 , loss 0.00060771214\n",
      "global_step 7601 , loss 0.00024143473\n",
      "global_step 7701 , loss 0.0002506282\n",
      "global_step 7801 , loss 0.0011355935\n",
      "global_step 7901 , loss 0.00023512653\n",
      "global_step 8001 , loss 0.00023964679\n",
      " true feats train [[ 0.          0.          0.          0.          0.         -0.23891878\n",
      "  -0.187608   -0.18181802 -0.22454418 -0.19324745 -0.17586286 -0.2145347\n",
      "  -0.17586312 -0.17586274 -0.18181802 -0.15681244  5.499992   -0.2570959\n",
      "  -0.27854282 -0.27854282 -0.19874883 -0.24356157 -0.27435157 -0.23420551\n",
      "  -0.2041239  -0.23891905 -0.24813494  0.          0.          0.\n",
      "   0.          0.         -0.78636384]\n",
      " [ 0.          0.          0.          0.          0.         -0.23891878\n",
      "  -0.187608   -0.18181802 -0.22454418 -0.19324745 -0.17586286 -0.2145347\n",
      "  -0.17586312 -0.17586274  5.4999948  -0.15681244 -0.18181793 -0.2570959\n",
      "  -0.27854282 -0.27854282 -0.19874883 -0.24356157 -0.27435157 -0.23420551\n",
      "  -0.2041239  -0.23891905 -0.24813494  0.          0.          0.\n",
      "   0.          0.          0.35518464]]\n",
      " true labels train [-0.51539123 -0.51539123  0.6626459   0.36813658  0.46630633 -0.3190517\n",
      " -0.51539123  0.46630633  0.46630633 -0.51539123]\n",
      " pred labels train [-0.5109889  -0.51355284  0.66441137  0.36958927  0.4604934  -0.31692642\n",
      " -0.4991309   0.47082525  0.47082525 -0.5159468 ]\n",
      " all vars norms [12.293268, 0.8210131, 16.305737, 0.32093573, 3.852072, 0.029978372]\n",
      "global_step 8101 , loss 0.00046821756\n",
      "global_step 8201 , loss 0.00023436465\n",
      "global_step 8301 , loss 0.00024347042\n",
      "global_step 8401 , loss 0.00024515908\n",
      "global_step 8501 , loss 0.00034308393\n",
      "global_step 8601 , loss 0.00049397495\n",
      "global_step 8701 , loss 0.003560017\n",
      "global_step 8801 , loss 0.00023637881\n",
      "global_step 8901 , loss 0.0002171073\n",
      "global_step 9001 , loss 0.00022064647\n",
      " true feats train [[ 0.          0.          0.          0.          0.         -0.23891887\n",
      "   5.330277   -0.18181807 -0.22454418 -0.19324704 -0.17586309 -0.21453469\n",
      "  -0.17586315 -0.17586292 -0.18181793 -0.15681244 -0.1818181  -0.25709614\n",
      "  -0.27854276 -0.27854276 -0.19874898 -0.24356152 -0.27435163 -0.23420528\n",
      "  -0.20412382 -0.23891887 -0.24813454  0.          0.          0.\n",
      "   0.          0.          1.4641175 ]\n",
      " [ 0.          0.          0.          0.          0.         -0.23891887\n",
      "  -0.1876081  -0.18181807 -0.22454418 -0.19324704 -0.17586309 -0.21453469\n",
      "  -0.17586315  5.686235   -0.18181793 -0.15681244 -0.1818181  -0.25709614\n",
      "  -0.27854276 -0.27854276 -0.19874898 -0.24356152 -0.27435163 -0.23420528\n",
      "  -0.20412382 -0.23891887 -0.24813454  0.          0.          0.\n",
      "   0.          0.         -1.275599  ]]\n",
      " true labels train [ 0.36813658 -0.3190517  -0.41722146 -0.22088195 -0.51539123  0.7608156\n",
      " -0.51539123  0.6626459   0.6626459   0.17179707]\n",
      " pred labels train [ 0.36674085 -0.31976423 -0.42076382 -0.32049355 -0.5171939   0.7589661\n",
      " -0.51965606  0.66060436  0.6645957   0.16879095]\n",
      " all vars norms [12.283406, 0.82026947, 16.357254, 0.32105473, 3.8527634, 0.030220319]\n",
      "global_step 9101 , loss 0.00021472239\n",
      "global_step 9201 , loss 0.00022765124\n",
      "global_step 9301 , loss 0.00026983177\n",
      "global_step 9401 , loss 0.000210502\n",
      "global_step 9501 , loss 0.00022954839\n",
      "global_step 9601 , loss 0.00023058864\n",
      "global_step 9701 , loss 0.00072489475\n",
      "global_step 9801 , loss 0.00032598444\n",
      "global_step 9901 , loss 0.0004263815\n",
      "global_step 10001 , loss 0.00020388946\n",
      " true feats train [[ 0.          0.          0.          0.          0.         -0.23891881\n",
      "  -0.18760797 -0.18181816 -0.22454418 -0.193247   -0.17586295 -0.21453469\n",
      "  -0.17586291 -0.17586288  5.4999943  -0.15681246 -0.18181804 -0.25709566\n",
      "  -0.27854273 -0.27854282 -0.1987489  -0.24356173 -0.2743515  -0.23420562\n",
      "  -0.20412375 -0.23891883 -0.24813482  0.          0.          0.\n",
      "   0.          0.         -0.52543855]\n",
      " [ 0.          0.          0.          0.          0.         -0.23891881\n",
      "  -0.18760797 -0.18181816 -0.22454418 -0.193247   -0.17586295 -0.21453469\n",
      "  -0.17586291 -0.17586288 -0.18181801 -0.15681246 -0.18181804 -0.25709566\n",
      "  -0.27854273 -0.27854282 -0.1987489   4.105755   -0.2743515  -0.23420562\n",
      "  -0.20412375 -0.23891883 -0.24813482  0.          0.          0.\n",
      "   0.          0.         -0.78636396]]\n",
      " true labels train [-0.51539123 -0.51539123 -0.51539123 -0.51539123 -0.51539123  0.9571551\n",
      "  0.36813658 -0.51539123 -0.51539123 -0.02454244]\n",
      " pred labels train [-0.5151957  -0.51504004 -0.5141336  -0.51502764 -0.5148314   0.96032715\n",
      "  0.36840236 -0.51929104 -0.5120326  -0.01978125]\n",
      " all vars norms [12.272168, 0.82030135, 16.399012, 0.3212471, 3.852976, 0.03041687]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-26f824a1dba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#     while True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         _, loss_val, global_step_val, true_feat_train, true_labels_train, pred_labels_train = sess.run(\n\u001b[0;32m---> 23\u001b[0;31m             [train_step, loss, global_step,  feature_batch, label_batch, out])\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    544\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1023\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m~/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ray-0321/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train.\n",
    "sess_args = {\n",
    "#              'checkpoint_dir': os.getcwd(),\n",
    "#              'save_checkpoint_secs': 60, \n",
    "             'save_summaries_secs': 1,\n",
    "             'log_step_count_steps': 5000,\n",
    "             'hooks': [tf.train.NanTensorHook(loss),\n",
    "                       tf.train.StopAtStepHook(last_step=2000000),\n",
    "#                       tf.train.LoggingTensorHook([global_step, loss], every_n_iter=500)\n",
    "                      ],\n",
    "            }\n",
    "\n",
    "loss_vals = collections.deque(maxlen=40)\n",
    "i = 0\n",
    "with tf.train.MonitoredTrainingSession(**sess_args) as sess:\n",
    "    sess.run(iterator_init)\n",
    "    while not sess.should_stop():\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     running_test_loss = 0.0\n",
    "#     while True:\n",
    "        _, loss_val, global_step_val, true_feat_train, true_labels_train, pred_labels_train = sess.run(\n",
    "            [train_step, loss, global_step,  feature_batch, label_batch, out])\n",
    "\n",
    "        if i % 100 == 0:\n",
    "#             sess.run(test_iterator_init)\n",
    "#             test_loss_val, actual_labels, predicted_labels = sess.run([loss, label_batch, out])\n",
    "\n",
    "#             loss_vals.append(test_loss_val)\n",
    "#             avg_test_loss = np.mean(loss_vals)\n",
    "#             print('global_step', global_step_val, ', loss', loss_val, ', avg test loss', avg_test_loss)\n",
    "            print('global_step', global_step_val, ', loss', loss_val)\n",
    "            if i % 1000 == 0:\n",
    "#                 print(' true labels', actual_labels[:20].reshape(-1,))\n",
    "#                 print(' pred labels', predicted_labels[:20].reshape(-1,))\n",
    "                print(' true feats train', true_feat_train[:2])\n",
    "                print(' true labels train', true_labels_train[:10].reshape(-1,))\n",
    "                print(' pred labels train', pred_labels_train[:10].reshape(-1,))\n",
    "                print(' all vars norms', sess.run(all_vars_norms))\n",
    "        i += 1\n",
    "#         if i >= 10000:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
